{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an XGboosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the appropriate packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n",
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost's hyperparameters\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). But the most common ones that you should know are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall parameters have been divided into 3 categories by XGBoost authors:\n",
    "\n",
    "- **General Parameters:** Guide the overall functioning\n",
    "- **Booster Parameters:** Guide the individual booster (tree/regression) at each step\n",
    "- **Learning Task Parameters:** Guide the optimization performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Parameters\n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "- **booster** [default=gbtree]\n",
    "Select the type of model to run at each iteration. It has 2 options:\n",
    "    - gbtree: tree-based models\n",
    "    - gblinear: linear models\n",
    "    \n",
    "- **silent** [default=0]:\n",
    "Silent mode is activated is set to 1, i.e. no running messages will be printed. It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "- **nthread**  [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "Though there are 2 types of boosters, we’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "\n",
    "- **eta [default=0.3]**\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "- **min_child_weight [default=1]**\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- **max_depth [default=6]**\n",
    "    - The maximum depth of a tree, same as GBM.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10\n",
    "- **max_leaf_nodes**\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "- **gamma [default=0]**\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "- **max_delta_step [default=0]**\n",
    "    - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    - This is generally not used but you can explore further if you wish.\n",
    "- **subsample [default=1]**\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bytree [default=1]**\n",
    "    - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bylevel [default=1]**\n",
    "    - Denotes the subsample ratio of columns for each split, in each level.\n",
    "    - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "- **lambda [default=1]**\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "- **alpha [default=0]**\n",
    "    - L1 regularization term on weight (analogous to Lasso regression)\n",
    "    - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "- **scale_pos_weight [default=1]**\n",
    "    - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Task Parameters\n",
    "\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "- **objective [default=reg:linear]**\n",
    "    - This defines the loss function to be minimized. Mostly used values are:\n",
    "        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "                - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "- **eval_metric [ default according to objective ]**\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "            - rmse – root mean square error\n",
    "            - mae – mean absolute error\n",
    "            - logloss – negative log-likelihood\n",
    "            - error – Binary classification error rate (0.5 threshold)\n",
    "            - merror – Multiclass classification error rate\n",
    "            - mlogloss – Multiclass logloss\n",
    "            - auc: Area under the curve\n",
    "- **seed [default=0]**\n",
    "    - The random number seed.\n",
    "    - Can be used for generating reproducible results and also for parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38245219347581555"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.5, \n",
    "                           subsample = 0.5,\n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 4, \n",
    "                           alpha = 1, \n",
    "                           #scale_pos_weight= titanic['Survived'].mean(),\n",
    "                           n_estimators = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.5, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=None, n_estimators=10000, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=0.5, verbosity=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.780269\n",
      "F1: 0.679739\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this link) like:\n",
    "\n",
    "- **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "- **metrics:** tells the evaluation metrics to be watched during CV\n",
    "- **as_pandas**: to return the results in a pandas DataFrame.\n",
    "- **early_stopping_rounds: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- **seed**: for reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running your model, you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    }
   ],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\":\"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 3, \n",
    "          'alpha': 1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=5,\n",
    "                    num_boost_round=500,\n",
    "                    early_stopping_rounds=5,\n",
    "                    metrics=\"logloss\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.659772</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.660168</td>\n",
       "      <td>0.001478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.639739</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.640184</td>\n",
       "      <td>0.010529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.627790</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.629019</td>\n",
       "      <td>0.010380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.614868</td>\n",
       "      <td>0.010608</td>\n",
       "      <td>0.616502</td>\n",
       "      <td>0.015193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.599252</td>\n",
       "      <td>0.010671</td>\n",
       "      <td>0.601503</td>\n",
       "      <td>0.014880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.372616</td>\n",
       "      <td>0.009112</td>\n",
       "      <td>0.434313</td>\n",
       "      <td>0.038028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.371920</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.434456</td>\n",
       "      <td>0.038454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.371195</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.434110</td>\n",
       "      <td>0.038592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.370798</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>0.434108</td>\n",
       "      <td>0.038613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.370584</td>\n",
       "      <td>0.009246</td>\n",
       "      <td>0.433982</td>\n",
       "      <td>0.038632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.659772           0.000850           0.660168   \n",
       "1              0.639739           0.006761           0.640184   \n",
       "2              0.627790           0.006070           0.629019   \n",
       "3              0.614868           0.010608           0.616502   \n",
       "4              0.599252           0.010671           0.601503   \n",
       "..                  ...                ...                ...   \n",
       "127            0.372616           0.009112           0.434313   \n",
       "128            0.371920           0.009004           0.434456   \n",
       "129            0.371195           0.009250           0.434110   \n",
       "130            0.370798           0.009382           0.434108   \n",
       "131            0.370584           0.009246           0.433982   \n",
       "\n",
       "     test-logloss-std  \n",
       "0            0.001478  \n",
       "1            0.010529  \n",
       "2            0.010380  \n",
       "3            0.015193  \n",
       "4            0.014880  \n",
       "..                ...  \n",
       "127          0.038028  \n",
       "128          0.038454  \n",
       "129          0.038592  \n",
       "130          0.038613  \n",
       "131          0.038632  \n",
       "\n",
       "[132 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "\n",
    "    return alg\n",
    "#     feat_imp = pd.Series(alg.get_booster().get_fscore())\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.concat([X_train, y_train], axis=1)\n",
    "target = 'Survived'\n",
    "IDcol = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>876</td>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>641</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>885</td>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  S  \\\n",
       "PassengerId                                                             \n",
       "740               3  24.0      0      0   7.8958        0     1  0  1   \n",
       "148               3   9.0      2      2  34.3750        1     0  0  1   \n",
       "876               3  15.0      0      0   7.2250        0     0  0  0   \n",
       "641               3  20.0      0      0   7.8542        0     1  0  1   \n",
       "885               3  25.0      0      0   7.0500        0     1  0  1   \n",
       "\n",
       "             Survived  \n",
       "PassengerId            \n",
       "740                 0  \n",
       "148                 0  \n",
       "876                 1  \n",
       "641                 0  \n",
       "885                 0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8919\n",
      "AUC Score (Train): 0.939281\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.3,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "alg = modelfit(xgb1, train, predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802691\n",
      "F1: 0.702703\n"
     ]
    }
   ],
   "source": [
    "preds = alg.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining XGBoost with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic')\n",
    "param_dist = {'n_estimators': [100,300,500],\n",
    "              'learning_rate': [0.07,0.05,0.03,0.01],\n",
    "              'max_depth': [3, 4, 5, 6, 7],\n",
    "              'colsample_bytree': [0.5,0.45,0.4],\n",
    "              'min_child_weight': [1, 2, 3]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the Gridsearch model\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = clf_xgb,\n",
    "    param_grid = param_dist, \n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    iid=False, \n",
    "    cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   28.7s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   52.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2700 out of 2700 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1, gamma=0,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=3, min_child_weight=1,\n",
       "                                     missing=None, n_estimators=100, n_jobs=1,\n",
       "                                     nthread=None, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=None, silent=None,\n",
       "                                     subsample=1, verbosity=1),\n",
       "             iid=False, n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.5, 0.45, 0.4],\n",
       "                         'learning_rate': [0.07, 0.05, 0.03, 0.01],\n",
       "                         'max_depth': [3, 4, 5, 6, 7],\n",
       "                         'min_child_weight': [1, 2, 3],\n",
       "                         'n_estimators': [100, 300, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.fit(train[predictors],train[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.06303864, 0.18986073, 0.28166685, 0.06114097, 0.17089348,\n",
       "        0.28284621, 0.0701232 , 0.17209859, 0.26913061, 0.06824255,\n",
       "        0.19788618, 0.32339807, 0.07134585, 0.19435582, 0.32135205,\n",
       "        0.07085562, 0.19588041, 0.34274793, 0.07835135, 0.22300801,\n",
       "        0.36274257, 0.0783462 , 0.21547952, 0.35447383, 0.07800298,\n",
       "        0.21022635, 0.34920177, 0.08570409, 0.24019856, 0.39888058,\n",
       "        0.08566885, 0.23676562, 0.38999639, 0.08621616, 0.23260822,\n",
       "        0.39977503, 0.09711452, 0.26286597, 0.42581596, 0.09426656,\n",
       "        0.26096659, 0.4228951 , 0.0884747 , 0.24784966, 0.41798038,\n",
       "        0.06315045, 0.176054  , 0.27817583, 0.06341214, 0.17306404,\n",
       "        0.29029241, 0.06391835, 0.17320118, 0.2882504 , 0.07183185,\n",
       "        0.20120616, 0.33205552, 0.06936059, 0.20778222, 0.33474283,\n",
       "        0.07235498, 0.20368886, 0.33215156, 0.08103862, 0.24725738,\n",
       "        0.4369348 , 0.10740891, 0.23576984, 0.3806345 , 0.07904353,\n",
       "        0.2239038 , 0.37392473, 0.09325924, 0.25747027, 0.43020301,\n",
       "        0.09409475, 0.27204638, 0.4239409 , 0.08804727, 0.25107613,\n",
       "        0.42422295, 0.10178704, 0.294485  , 0.47372637, 0.09871016,\n",
       "        0.29011912, 0.46720424, 0.09820962, 0.27071071, 0.51153879,\n",
       "        0.07040982, 0.19161901, 0.31099248, 0.06596937, 0.18749175,\n",
       "        0.30814404, 0.06699982, 0.18541894, 0.31066604, 0.07862616,\n",
       "        0.21688495, 0.36032777, 0.07991681, 0.22139077, 0.36483722,\n",
       "        0.07684464, 0.21825762, 0.35913253, 0.08650661, 0.24515629,\n",
       "        0.4066431 , 0.08566875, 0.24693422, 0.40650277, 0.08642735,\n",
       "        0.24896827, 0.42027001, 0.1088367 , 0.34839711, 0.57211051,\n",
       "        0.10234699, 0.36145082, 0.59131894, 0.12685075, 0.38581104,\n",
       "        0.60620313, 0.15166507, 0.32184958, 0.545081  , 0.11632371,\n",
       "        0.32578301, 0.50322895, 0.10433235, 0.29696636, 0.48461227,\n",
       "        0.07182899, 0.2123847 , 0.32744493, 0.06903133, 0.20376792,\n",
       "        0.33289266, 0.07445712, 0.19774761, 0.34005365, 0.08917336,\n",
       "        0.23217955, 0.38173594, 0.08144155, 0.23311071, 0.37761021,\n",
       "        0.07951198, 0.2223774 , 0.37706308, 0.08958197, 0.27067471,\n",
       "        0.43374557, 0.08783903, 0.26215181, 0.44333205, 0.0961226 ,\n",
       "        0.25299349, 0.42409391, 0.10236259, 0.28688421, 0.47487335,\n",
       "        0.09927206, 0.28108673, 0.47301679, 0.09628158, 0.2832376 ,\n",
       "        0.46713009, 0.10806346, 0.33670278, 0.54532042, 0.11091576,\n",
       "        0.30953259, 0.51524911, 0.10142593, 0.3153852 , 0.49635534,\n",
       "        0.07070746, 0.19558778, 0.32709155, 0.07057438, 0.19883108,\n",
       "        0.33481717, 0.07279263, 0.20964608, 0.32007608, 0.07912097,\n",
       "        0.23001761, 0.38531241, 0.08276453, 0.23106833, 0.3826376 ,\n",
       "        0.08184071, 0.22765594, 0.38061976, 0.09094744, 0.26071925,\n",
       "        0.43450141, 0.09117012, 0.25921645, 0.42785892, 0.09834561,\n",
       "        0.26004801, 0.41438975, 0.10165782, 0.28632555, 0.48409114,\n",
       "        0.10459709, 0.28863387, 0.49205685, 0.10370674, 0.29459858,\n",
       "        0.49483232, 0.11453047, 0.34158087, 0.57791123, 0.1277451 ,\n",
       "        0.35081911, 0.54496522, 0.11511464, 0.31401696, 0.50083675,\n",
       "        0.07396207, 0.20821218, 0.33966475, 0.07292657, 0.20748615,\n",
       "        0.33334827, 0.07523699, 0.20616469, 0.33836198, 0.08642473,\n",
       "        0.24288282, 0.39651494, 0.08419504, 0.23708849, 0.39408526,\n",
       "        0.08241239, 0.24903159, 0.38864594, 0.09607306, 0.26753697,\n",
       "        0.44791827, 0.09324098, 0.26857047, 0.44394484, 0.09890709,\n",
       "        0.2684628 , 0.42622342, 0.10227375, 0.3222878 , 0.53409934,\n",
       "        0.10251369, 0.28560162, 0.49602294, 0.10503039, 0.2887743 ,\n",
       "        0.47200265, 0.10994363, 0.32376409, 0.53141842, 0.111555  ,\n",
       "        0.3154429 , 0.50682645, 0.10948992, 0.32527566, 0.50102811,\n",
       "        0.07387476, 0.21239443, 0.34019675, 0.07192817, 0.21501646,\n",
       "        0.3513464 , 0.0729094 , 0.21203427, 0.34146099, 0.0886672 ,\n",
       "        0.24437928, 0.40279961, 0.08395834, 0.23950129, 0.39253488,\n",
       "        0.08502736, 0.23430667, 0.39241672, 0.09877248, 0.27607427,\n",
       "        0.45352106, 0.09525404, 0.29260502, 0.55095081, 0.11300049,\n",
       "        0.2637435 , 0.4723331 , 0.13087649, 0.36170015, 0.61495066,\n",
       "        0.13278761, 0.32169256, 0.54909821, 0.10856929, 0.34022131,\n",
       "        0.54862595, 0.11976309, 0.35143642, 0.55797458, 0.10933022,\n",
       "        0.32745152, 0.53870049, 0.10919957, 0.31300297, 0.5234374 ,\n",
       "        0.07285914, 0.22449546, 0.38075318, 0.07460308, 0.20211458,\n",
       "        0.34600606, 0.0744163 , 0.20786123, 0.34526005, 0.09244018,\n",
       "        0.25234985, 0.46320758, 0.08812633, 0.24375024, 0.447544  ,\n",
       "        0.11867681, 0.26663465, 0.39760604, 0.10238032, 0.31292906,\n",
       "        0.4675642 , 0.107409  , 0.28588796, 0.4540731 , 0.0995995 ,\n",
       "        0.27829213, 0.44515924, 0.10592265, 0.30665441, 0.50724301,\n",
       "        0.10032449, 0.3019218 , 0.49423881, 0.10189748, 0.29137039,\n",
       "        0.48756609, 0.11052442, 0.36724319, 0.65692153, 0.13401299,\n",
       "        0.41251678, 0.75405731, 0.13649907, 0.38774405, 0.582656  ,\n",
       "        0.07146087, 0.21933393, 0.34952846, 0.07182159, 0.19603381,\n",
       "        0.32995567, 0.07095218, 0.19302101, 0.31497111, 0.07685866,\n",
       "        0.21561437, 0.35593801, 0.07893   , 0.21883149, 0.35669203,\n",
       "        0.07472425, 0.21766286, 0.3599474 , 0.08410478, 0.23729911,\n",
       "        0.39024129, 0.08420944, 0.236449  , 0.39246945, 0.0834219 ,\n",
       "        0.23539906, 0.40620708, 0.10273786, 0.25563674, 0.42867765,\n",
       "        0.08999224, 0.25363312, 0.43516207, 0.08922648, 0.24738255,\n",
       "        0.41174302, 0.09735441, 0.27663121, 0.4590992 , 0.0945262 ,\n",
       "        0.27011409, 0.43587737, 0.09433675, 0.26531696, 0.43431716,\n",
       "        0.06810374, 0.1941484 , 0.33013754, 0.06924663, 0.19713154,\n",
       "        0.31538939, 0.06795082, 0.20069494, 0.32462459, 0.0819418 ,\n",
       "        0.22198062, 0.36499195, 0.08409505, 0.21870847, 0.37139721,\n",
       "        0.07470474, 0.21403074, 0.36069307, 0.0847569 , 0.24818773,\n",
       "        0.41034045, 0.08930097, 0.25750294, 0.409446  , 0.08503118,\n",
       "        0.24494934, 0.39341359, 0.09222364, 0.26406269, 0.43063173,\n",
       "        0.09053884, 0.25537691, 0.42627006, 0.08773184, 0.24900575,\n",
       "        0.41001682, 0.09735661, 0.2717473 , 0.4637208 , 0.09676228,\n",
       "        0.26783619, 0.44365416, 0.09531012, 0.26112199, 0.43384695,\n",
       "        0.06901073, 0.19320283, 0.32033019, 0.06842141, 0.19651895,\n",
       "        0.32134018, 0.07091265, 0.1938252 , 0.32339501, 0.07963371,\n",
       "        0.22408524, 0.36140203, 0.0784369 , 0.22127166, 0.35750618,\n",
       "        0.07646141, 0.21956921, 0.36098552, 0.08513422, 0.24273782,\n",
       "        0.4061739 , 0.08536592, 0.24325986, 0.39543648, 0.08333912,\n",
       "        0.23747025, 0.40536485, 0.09308968, 0.25299616, 0.43168359,\n",
       "        0.09428701, 0.25482678, 0.43521523, 0.09978685, 0.25701928,\n",
       "        0.42113295, 0.10014906, 0.27957697, 0.46337805, 0.09842601,\n",
       "        0.27431493, 0.44541698, 0.09363799, 0.26874208, 0.43896546,\n",
       "        0.06819844, 0.20001597, 0.32975745, 0.07012615, 0.19215455,\n",
       "        0.31884561, 0.06882687, 0.19863925, 0.32297277, 0.07858338,\n",
       "        0.22112975, 0.36117492, 0.08008509, 0.217664  , 0.39243059,\n",
       "        0.08816857, 0.22278948, 0.36584911, 0.08764424, 0.24693971,\n",
       "        0.40989752, 0.08683991, 0.25136261, 0.40374355, 0.08211479,\n",
       "        0.24352398, 0.40066185, 0.09390855, 0.26214781, 0.43338737,\n",
       "        0.08894839, 0.2602365 , 0.42784581, 0.08835382, 0.24946399,\n",
       "        0.46089683, 0.09689302, 0.27197857, 0.46717844, 0.09632807,\n",
       "        0.28177948, 0.46342759, 0.09507694, 0.26738224, 0.39969406]),\n",
       " 'std_fit_time': array([0.00363159, 0.00589384, 0.00561158, 0.00271502, 0.00475967,\n",
       "        0.01444779, 0.00801058, 0.00457687, 0.0048512 , 0.00310816,\n",
       "        0.00521389, 0.00400227, 0.00908603, 0.01310002, 0.00631101,\n",
       "        0.00309725, 0.00979016, 0.0157182 , 0.00271359, 0.0096198 ,\n",
       "        0.0061603 , 0.00324755, 0.00545508, 0.00533375, 0.00365529,\n",
       "        0.00549762, 0.00532279, 0.00367959, 0.00616451, 0.0057603 ,\n",
       "        0.0029625 , 0.0053933 , 0.0057147 , 0.00290198, 0.00356999,\n",
       "        0.00745883, 0.00351535, 0.00609064, 0.01425657, 0.00401315,\n",
       "        0.0033131 , 0.00906057, 0.00346873, 0.00733187, 0.01403783,\n",
       "        0.00444533, 0.00493902, 0.00548739, 0.00313757, 0.00440884,\n",
       "        0.0053568 , 0.00372577, 0.00314102, 0.00625074, 0.0029556 ,\n",
       "        0.00271813, 0.00488477, 0.00154929, 0.00270363, 0.0045649 ,\n",
       "        0.00169526, 0.00376688, 0.00581968, 0.00288951, 0.00803331,\n",
       "        0.04231898, 0.00611743, 0.00484619, 0.00852769, 0.00194546,\n",
       "        0.00867107, 0.00423664, 0.00372017, 0.00293284, 0.00915823,\n",
       "        0.00751208, 0.00837737, 0.01230585, 0.00256907, 0.0051467 ,\n",
       "        0.01640735, 0.00406811, 0.00715195, 0.00647967, 0.00316115,\n",
       "        0.01400685, 0.0132669 , 0.00403208, 0.00386396, 0.0162921 ,\n",
       "        0.0061706 , 0.0032038 , 0.00394861, 0.00118717, 0.00304343,\n",
       "        0.00653469, 0.00033456, 0.00533142, 0.00916282, 0.00209166,\n",
       "        0.00406024, 0.00952765, 0.00656649, 0.00696433, 0.00426767,\n",
       "        0.002541  , 0.00825326, 0.00227432, 0.00204086, 0.00305363,\n",
       "        0.00768027, 0.00146381, 0.00528283, 0.00394   , 0.0023128 ,\n",
       "        0.00532375, 0.01353162, 0.01442248, 0.01919523, 0.02804977,\n",
       "        0.00387083, 0.0386926 , 0.02510087, 0.00322301, 0.00901842,\n",
       "        0.0386419 , 0.0126774 , 0.00904569, 0.01951777, 0.00998883,\n",
       "        0.02074935, 0.01315958, 0.00785741, 0.00490586, 0.01424731,\n",
       "        0.00139643, 0.00668247, 0.00605224, 0.00203348, 0.00271337,\n",
       "        0.01350668, 0.00569233, 0.00441669, 0.01012423, 0.01084676,\n",
       "        0.00971999, 0.0091059 , 0.00256129, 0.00447202, 0.00605299,\n",
       "        0.00330924, 0.00359011, 0.00656134, 0.0027772 , 0.00885317,\n",
       "        0.00257477, 0.00164774, 0.00582545, 0.00941841, 0.00553233,\n",
       "        0.00548746, 0.00577837, 0.00299351, 0.00480664, 0.00252528,\n",
       "        0.00314551, 0.00411547, 0.01273418, 0.0043065 , 0.00516239,\n",
       "        0.00405207, 0.00327137, 0.04984793, 0.00461169, 0.00866854,\n",
       "        0.00604827, 0.00681467, 0.00260312, 0.00838476, 0.007345  ,\n",
       "        0.00279453, 0.00325199, 0.00708154, 0.00262569, 0.00554044,\n",
       "        0.00741004, 0.00251136, 0.00428161, 0.00950797, 0.00209423,\n",
       "        0.0056337 , 0.00708375, 0.00256096, 0.00679355, 0.01256798,\n",
       "        0.00268839, 0.00541586, 0.00433765, 0.00243181, 0.00750019,\n",
       "        0.00978773, 0.00470939, 0.00475182, 0.00744924, 0.00672068,\n",
       "        0.01472009, 0.0099854 , 0.00359921, 0.00721169, 0.01930488,\n",
       "        0.00488509, 0.00538683, 0.01543563, 0.00269895, 0.01241438,\n",
       "        0.05797942, 0.00818768, 0.01558858, 0.01531973, 0.01117686,\n",
       "        0.02921166, 0.01111997, 0.004277  , 0.02006998, 0.00361925,\n",
       "        0.00520239, 0.00634726, 0.01252393, 0.00263925, 0.00835375,\n",
       "        0.00362518, 0.00572247, 0.00167648, 0.00498185, 0.0032832 ,\n",
       "        0.00099235, 0.00686824, 0.00373082, 0.00308337, 0.00714334,\n",
       "        0.00306529, 0.00581327, 0.00579182, 0.00168076, 0.00682349,\n",
       "        0.00946453, 0.00220872, 0.00890748, 0.0142945 , 0.00653134,\n",
       "        0.00745636, 0.00207963, 0.00290475, 0.04664361, 0.03083357,\n",
       "        0.00384617, 0.00308892, 0.02372863, 0.00509345, 0.00196282,\n",
       "        0.00427438, 0.00257224, 0.00899216, 0.01184441, 0.00509054,\n",
       "        0.00420948, 0.00715259, 0.00328884, 0.01015283, 0.01053403,\n",
       "        0.00384298, 0.00350836, 0.00414436, 0.00219099, 0.01602907,\n",
       "        0.00848545, 0.00176332, 0.00491105, 0.01037505, 0.00492275,\n",
       "        0.00380419, 0.00529935, 0.00145736, 0.0075649 , 0.01075215,\n",
       "        0.00188522, 0.00364605, 0.0128313 , 0.00493329, 0.00706246,\n",
       "        0.00287211, 0.00196605, 0.01644471, 0.04708243, 0.00736667,\n",
       "        0.00404908, 0.0278177 , 0.02372865, 0.01805491, 0.04952938,\n",
       "        0.01320789, 0.01209155, 0.04260486, 0.00487415, 0.01135223,\n",
       "        0.03624065, 0.01152818, 0.03338717, 0.0190071 , 0.00393825,\n",
       "        0.01695223, 0.01276142, 0.00238417, 0.00778562, 0.00614426,\n",
       "        0.00250178, 0.01428511, 0.01619298, 0.00398688, 0.00222272,\n",
       "        0.00679082, 0.00383867, 0.00572366, 0.00558916, 0.01006329,\n",
       "        0.01756944, 0.02462385, 0.00410831, 0.00436223, 0.03577717,\n",
       "        0.04287937, 0.04046631, 0.013684  , 0.00591027, 0.0070666 ,\n",
       "        0.02794917, 0.01548931, 0.01180425, 0.01433081, 0.00799238,\n",
       "        0.00648734, 0.01039033, 0.00587998, 0.00523698, 0.00754857,\n",
       "        0.00467027, 0.01270828, 0.01383582, 0.00340832, 0.00489426,\n",
       "        0.01709513, 0.00309673, 0.02093012, 0.01680009, 0.01848868,\n",
       "        0.02217555, 0.0466331 , 0.00577021, 0.01748182, 0.02615993,\n",
       "        0.00462257, 0.0158182 , 0.01234388, 0.00555491, 0.00464093,\n",
       "        0.00571662, 0.00286946, 0.00347437, 0.00390732, 0.00337828,\n",
       "        0.00380724, 0.01184985, 0.00750506, 0.00255367, 0.00484943,\n",
       "        0.00171606, 0.00495124, 0.00564534, 0.0018628 , 0.00362779,\n",
       "        0.00513213, 0.00606586, 0.00393496, 0.00588567, 0.00368187,\n",
       "        0.00407224, 0.02078941, 0.00576445, 0.00164361, 0.0078967 ,\n",
       "        0.00320049, 0.00231238, 0.00966497, 0.00181819, 0.00531095,\n",
       "        0.00373252, 0.00196205, 0.00482114, 0.00603151, 0.00278539,\n",
       "        0.00233688, 0.00595751, 0.00318152, 0.00300438, 0.01029223,\n",
       "        0.00118713, 0.00380849, 0.00655112, 0.00274443, 0.00640321,\n",
       "        0.01517008, 0.00364913, 0.01111924, 0.00871345, 0.00351778,\n",
       "        0.00419584, 0.01657708, 0.00743717, 0.00667355, 0.00577503,\n",
       "        0.00097564, 0.00303208, 0.01161681, 0.00214157, 0.0132254 ,\n",
       "        0.00934611, 0.00730332, 0.0071931 , 0.00638322, 0.00453364,\n",
       "        0.00443258, 0.0051214 , 0.00395176, 0.00687057, 0.00760508,\n",
       "        0.00313791, 0.00452602, 0.0166333 , 0.00288272, 0.00741886,\n",
       "        0.00417949, 0.00312799, 0.00552172, 0.00718365, 0.00212103,\n",
       "        0.00308898, 0.00509533, 0.00364249, 0.00530008, 0.00189936,\n",
       "        0.0017613 , 0.00219245, 0.00903101, 0.00092134, 0.00315499,\n",
       "        0.00578387, 0.0036869 , 0.0032332 , 0.00388605, 0.00266486,\n",
       "        0.00240361, 0.00189197, 0.00346065, 0.0023225 , 0.00318991,\n",
       "        0.0032584 , 0.00596525, 0.0031635 , 0.00270315, 0.00686122,\n",
       "        0.01106353, 0.00352563, 0.00829425, 0.00401027, 0.001976  ,\n",
       "        0.00754358, 0.00777514, 0.00230922, 0.00164972, 0.00423802,\n",
       "        0.00650661, 0.00393786, 0.01385099, 0.00737305, 0.00643121,\n",
       "        0.00552441, 0.00436823, 0.00978745, 0.00465816, 0.0073664 ,\n",
       "        0.00832246, 0.00908409, 0.00313807, 0.00614058, 0.00855264,\n",
       "        0.0018615 , 0.00635961, 0.00746309, 0.00273404, 0.0053246 ,\n",
       "        0.00512855, 0.00242449, 0.00589152, 0.00854103, 0.00334619,\n",
       "        0.00307267, 0.00883505, 0.00290607, 0.00093369, 0.02048221,\n",
       "        0.01561526, 0.00319405, 0.00634868, 0.00332862, 0.00487993,\n",
       "        0.0127791 , 0.00604109, 0.01107672, 0.00756157, 0.00163926,\n",
       "        0.00991836, 0.00741048, 0.00272896, 0.00312307, 0.00187009,\n",
       "        0.00079532, 0.00981422, 0.00384482, 0.00245438, 0.00426539,\n",
       "        0.01760514, 0.00184784, 0.00597292, 0.00336612, 0.00213045,\n",
       "        0.00949723, 0.01307368, 0.00496967, 0.00866895, 0.06343452]),\n",
       " 'mean_score_time': array([0.00392542, 0.00649681, 0.0054729 , 0.00357885, 0.004877  ,\n",
       "        0.00783515, 0.00432076, 0.00465527, 0.00560865, 0.00424018,\n",
       "        0.00500703, 0.00700765, 0.00400085, 0.00520139, 0.00634713,\n",
       "        0.00372534, 0.00516925, 0.00656061, 0.00452781, 0.00557604,\n",
       "        0.00742402, 0.00415187, 0.00520411, 0.00695581, 0.00444093,\n",
       "        0.00517149, 0.00710864, 0.00383935, 0.00638857, 0.00806041,\n",
       "        0.00381131, 0.00572214, 0.00799174, 0.00385475, 0.00540648,\n",
       "        0.00824566, 0.00444632, 0.00599518, 0.00867305, 0.00485759,\n",
       "        0.00630159, 0.008641  , 0.00387101, 0.00635457, 0.00873914,\n",
       "        0.00359459, 0.00534539, 0.00648108, 0.00373001, 0.0052547 ,\n",
       "        0.00576682, 0.003618  , 0.00445523, 0.00626507, 0.00409818,\n",
       "        0.00576396, 0.00702505, 0.00370007, 0.00532408, 0.00671954,\n",
       "        0.00384221, 0.00524626, 0.00651455, 0.00458441, 0.00565972,\n",
       "        0.00846081, 0.00890012, 0.00630498, 0.00743961, 0.0039608 ,\n",
       "        0.00545564, 0.00744705, 0.00398855, 0.00598083, 0.00851789,\n",
       "        0.00405817, 0.00636396, 0.00803132, 0.00407338, 0.00590372,\n",
       "        0.00793352, 0.00448456, 0.00825577, 0.01038008, 0.00433879,\n",
       "        0.00663581, 0.00898399, 0.00432558, 0.00752163, 0.01101546,\n",
       "        0.00383401, 0.00584731, 0.00590687, 0.00363712, 0.00497031,\n",
       "        0.00632758, 0.00381846, 0.00523987, 0.00637736, 0.00478315,\n",
       "        0.00673814, 0.00702519, 0.00401688, 0.00646005, 0.00799441,\n",
       "        0.00487814, 0.00572076, 0.00725689, 0.00564079, 0.00652142,\n",
       "        0.00812883, 0.00404296, 0.00586095, 0.00820322, 0.00410604,\n",
       "        0.00606556, 0.00840421, 0.00625248, 0.01053386, 0.01180682,\n",
       "        0.00429049, 0.00795636, 0.01182823, 0.00529842, 0.01236801,\n",
       "        0.01074367, 0.00835142, 0.00913382, 0.01195927, 0.00441175,\n",
       "        0.00672092, 0.00921259, 0.0056675 , 0.00646658, 0.01116781,\n",
       "        0.0044302 , 0.00515809, 0.00640144, 0.00425215, 0.00803528,\n",
       "        0.00621252, 0.00822291, 0.00529575, 0.00729928, 0.00666003,\n",
       "        0.00579467, 0.00841284, 0.00426826, 0.00586891, 0.00851755,\n",
       "        0.00403919, 0.00552278, 0.01107874, 0.00420265, 0.00627589,\n",
       "        0.00821505, 0.00410876, 0.00653353, 0.00827537, 0.00437632,\n",
       "        0.00606647, 0.00926275, 0.00440507, 0.0066226 , 0.00912766,\n",
       "        0.0044446 , 0.00634103, 0.00892901, 0.00444636, 0.00676632,\n",
       "        0.00867801, 0.00477457, 0.00724087, 0.00972662, 0.00448723,\n",
       "        0.00662761, 0.01014938, 0.00450382, 0.00659485, 0.00906663,\n",
       "        0.00381474, 0.00561924, 0.00650096, 0.0040566 , 0.00480671,\n",
       "        0.00703607, 0.00442915, 0.00521483, 0.00637751, 0.00413609,\n",
       "        0.00587311, 0.00770154, 0.0042892 , 0.00617728, 0.00766821,\n",
       "        0.00411367, 0.00587845, 0.00794396, 0.00430822, 0.00635476,\n",
       "        0.00872378, 0.00421429, 0.00632997, 0.00855441, 0.00440388,\n",
       "        0.00610752, 0.00900311, 0.00643182, 0.00680857, 0.01054659,\n",
       "        0.00431647, 0.00752254, 0.00908246, 0.00465908, 0.00682645,\n",
       "        0.01014924, 0.00443592, 0.00961823, 0.01594105, 0.00511622,\n",
       "        0.00768566, 0.01237745, 0.0054451 , 0.00698843, 0.0101562 ,\n",
       "        0.00446658, 0.00632315, 0.00708151, 0.00522256, 0.00623941,\n",
       "        0.00663218, 0.00491662, 0.00544086, 0.00677323, 0.00535946,\n",
       "        0.00586677, 0.00932112, 0.00469594, 0.00612411, 0.00797215,\n",
       "        0.00428567, 0.00607195, 0.00781784, 0.00506816, 0.00636253,\n",
       "        0.00894513, 0.00440488, 0.0065752 , 0.01112294, 0.00425191,\n",
       "        0.00640154, 0.00844493, 0.00563149, 0.01112518, 0.00947623,\n",
       "        0.00450873, 0.00693974, 0.00930476, 0.00459127, 0.00677066,\n",
       "        0.00873137, 0.00454502, 0.00763907, 0.01049466, 0.00466423,\n",
       "        0.00702505, 0.01003332, 0.00487356, 0.00764985, 0.01096768,\n",
       "        0.00413289, 0.00643678, 0.00636277, 0.00516343, 0.0074852 ,\n",
       "        0.00779676, 0.00405817, 0.00553122, 0.00702343, 0.00449286,\n",
       "        0.00615964, 0.00747261, 0.00407543, 0.00601001, 0.00778165,\n",
       "        0.00457234, 0.00637074, 0.00769196, 0.00473537, 0.00681639,\n",
       "        0.00901265, 0.00438733, 0.01210065, 0.00908442, 0.00512071,\n",
       "        0.00780334, 0.01036634, 0.00453353, 0.01018791, 0.01125517,\n",
       "        0.00656686, 0.00783858, 0.01222038, 0.00481739, 0.00818315,\n",
       "        0.0090178 , 0.00647607, 0.00728188, 0.01046181, 0.00449514,\n",
       "        0.00697365, 0.00970545, 0.00469284, 0.00689478, 0.00922647,\n",
       "        0.00385189, 0.00590706, 0.00660381, 0.00409522, 0.00532455,\n",
       "        0.00719566, 0.00423751, 0.00544243, 0.01088204, 0.00498686,\n",
       "        0.00736513, 0.00894518, 0.00420365, 0.00604515, 0.00860682,\n",
       "        0.00777121, 0.00581551, 0.00973039, 0.00470262, 0.00729208,\n",
       "        0.00970502, 0.00626025, 0.00671186, 0.00884023, 0.00446348,\n",
       "        0.00634375, 0.00837336, 0.00509143, 0.00693903, 0.01019006,\n",
       "        0.00447679, 0.00706735, 0.01097903, 0.00497289, 0.00689278,\n",
       "        0.00904245, 0.00450244, 0.00866466, 0.01199741, 0.00569868,\n",
       "        0.00943341, 0.01388178, 0.00496039, 0.00884442, 0.0094552 ,\n",
       "        0.00435772, 0.00684309, 0.00661478, 0.00472832, 0.00578456,\n",
       "        0.00696387, 0.00428104, 0.0056139 , 0.00722742, 0.00452075,\n",
       "        0.00578012, 0.00849853, 0.00471497, 0.00581961, 0.00734816,\n",
       "        0.00486178, 0.00642171, 0.00748343, 0.00458088, 0.00739999,\n",
       "        0.00825267, 0.00426664, 0.00614262, 0.00832996, 0.00486593,\n",
       "        0.0068017 , 0.01021929, 0.00811172, 0.00660086, 0.00982676,\n",
       "        0.00503592, 0.00658631, 0.00970297, 0.00441899, 0.00662241,\n",
       "        0.00947413, 0.00475602, 0.00703092, 0.0108263 , 0.00474505,\n",
       "        0.00752015, 0.01014838, 0.00462494, 0.00678859, 0.00991902,\n",
       "        0.00419402, 0.00530486, 0.00702505, 0.00423536, 0.00564623,\n",
       "        0.00762515, 0.00490704, 0.00520887, 0.00663075, 0.00522084,\n",
       "        0.0062284 , 0.00835814, 0.00544577, 0.00580058, 0.00745807,\n",
       "        0.00412488, 0.00619578, 0.0096014 , 0.00450382, 0.00706897,\n",
       "        0.01091623, 0.00433416, 0.00639305, 0.00830579, 0.00464067,\n",
       "        0.00646629, 0.00828328, 0.00447893, 0.0068902 , 0.00901875,\n",
       "        0.00483584, 0.00674524, 0.0095716 , 0.00436559, 0.00653825,\n",
       "        0.00878477, 0.00507221, 0.00732908, 0.01002464, 0.00477638,\n",
       "        0.00722647, 0.00928693, 0.00447536, 0.00688682, 0.00913024,\n",
       "        0.00401211, 0.00508895, 0.00656352, 0.00436316, 0.00532942,\n",
       "        0.00691724, 0.00503979, 0.00518484, 0.00709996, 0.00441918,\n",
       "        0.00647459, 0.00752134, 0.00513177, 0.00596089, 0.0083313 ,\n",
       "        0.00423179, 0.00632386, 0.007974  , 0.00433602, 0.00868979,\n",
       "        0.00941668, 0.00466228, 0.00643134, 0.0087276 , 0.00419917,\n",
       "        0.0064693 , 0.00937629, 0.00466456, 0.00663309, 0.00966935,\n",
       "        0.00447235, 0.00707474, 0.00969205, 0.00488586, 0.00724478,\n",
       "        0.00881524, 0.00466461, 0.00701923, 0.01064138, 0.00450983,\n",
       "        0.00678883, 0.00914645, 0.00469217, 0.00688753, 0.00922179,\n",
       "        0.00433469, 0.00558791, 0.00720921, 0.00414896, 0.00516567,\n",
       "        0.00735798, 0.00422354, 0.0053412 , 0.00703058, 0.00430117,\n",
       "        0.00569096, 0.0077908 , 0.00471311, 0.00693717, 0.00774536,\n",
       "        0.00434566, 0.00646939, 0.007724  , 0.0045505 , 0.00646029,\n",
       "        0.00951099, 0.00603371, 0.00629706, 0.00844483, 0.00428987,\n",
       "        0.00673966, 0.00840616, 0.00467558, 0.0068234 , 0.00994616,\n",
       "        0.00432796, 0.0063817 , 0.0088903 , 0.00433602, 0.0072082 ,\n",
       "        0.00867567, 0.00466623, 0.00694752, 0.0114439 , 0.00493064,\n",
       "        0.00689955, 0.00930643, 0.00441904, 0.00701976, 0.00722976]),\n",
       " 'std_score_time': array([4.44899995e-04, 3.44751047e-03, 1.50343361e-04, 1.70383959e-04,\n",
       "        5.31148279e-04, 4.87782651e-03, 1.78273423e-03, 4.09248102e-04,\n",
       "        2.32548676e-04, 1.31481969e-03, 2.30727700e-04, 8.29618678e-04,\n",
       "        8.71193825e-04, 4.69542724e-04, 1.52469037e-04, 2.67811862e-04,\n",
       "        5.28122664e-04, 3.30352267e-04, 1.75978435e-03, 3.35714037e-04,\n",
       "        3.51480858e-04, 8.42582004e-04, 2.74455307e-05, 1.84171363e-04,\n",
       "        1.42963564e-03, 1.38235281e-04, 5.12626235e-04, 2.20672313e-04,\n",
       "        1.15444426e-03, 4.66777533e-04, 1.68069602e-04, 1.96855659e-04,\n",
       "        5.26329723e-04, 1.60156534e-04, 1.61700226e-04, 1.83965237e-03,\n",
       "        6.69406404e-04, 1.08994127e-04, 8.28326489e-04, 1.73949911e-03,\n",
       "        6.66910276e-04, 1.40335621e-03, 4.52406149e-05, 1.32302739e-03,\n",
       "        1.65203169e-03, 3.22798907e-04, 7.42176188e-04, 1.52833903e-03,\n",
       "        6.03534232e-04, 7.07666545e-04, 1.63870235e-04, 2.14113955e-04,\n",
       "        7.52349612e-05, 5.15871202e-04, 8.33870785e-04, 1.42152901e-03,\n",
       "        8.15735730e-04, 9.80905611e-05, 2.39581771e-04, 9.96479457e-05,\n",
       "        4.84222032e-04, 3.38013936e-04, 1.35540910e-04, 1.55961335e-03,\n",
       "        1.58467530e-04, 1.02287469e-03, 4.64225323e-03, 1.15310557e-03,\n",
       "        1.46873740e-04, 2.84935176e-04, 4.45636406e-05, 1.72098345e-04,\n",
       "        5.43377649e-05, 2.51158492e-04, 8.84892637e-04, 1.94378876e-04,\n",
       "        3.18478556e-04, 2.36101305e-04, 2.27761108e-04, 1.35571773e-04,\n",
       "        2.14333746e-04, 5.02560658e-04, 2.44949699e-03, 2.62482005e-03,\n",
       "        3.60681511e-04, 3.62553111e-04, 5.13624201e-04, 2.73654573e-04,\n",
       "        2.64933758e-03, 5.10452628e-03, 1.73884474e-04, 1.62992558e-03,\n",
       "        1.06718819e-04, 9.07216247e-05, 2.73480639e-04, 7.02693556e-04,\n",
       "        1.82991464e-04, 5.41704528e-04, 6.15966156e-04, 1.80377096e-03,\n",
       "        2.71930723e-03, 2.30301606e-04, 1.58207858e-04, 2.16040149e-03,\n",
       "        1.35713387e-03, 1.86604086e-03, 6.08014939e-04, 6.35413874e-04,\n",
       "        2.71437378e-03, 9.69848430e-04, 3.55557917e-04, 1.50548947e-04,\n",
       "        9.56578125e-05, 4.21727501e-04, 2.28158633e-04, 2.25171847e-04,\n",
       "        7.11454741e-04, 3.49047492e-03, 3.65980201e-03, 3.18881626e-03,\n",
       "        2.16255645e-04, 2.26584900e-03, 4.06154686e-03, 6.19114106e-04,\n",
       "        9.12816603e-03, 2.78315268e-03, 3.42955024e-03, 2.17216854e-03,\n",
       "        3.94188301e-03, 1.01981122e-04, 9.69585486e-05, 2.02624072e-04,\n",
       "        2.43372250e-03, 2.05876419e-04, 2.33538695e-03, 5.72500661e-04,\n",
       "        4.56957221e-04, 2.25554893e-04, 7.63457635e-04, 3.27779718e-03,\n",
       "        8.80166223e-05, 4.45997090e-03, 4.65464171e-04, 1.94236293e-03,\n",
       "        5.07027593e-03, 4.84512324e-04, 2.05370668e-03, 4.03107445e-04,\n",
       "        6.15533479e-04, 2.50928437e-03, 7.67452482e-05, 1.56824224e-04,\n",
       "        6.07788919e-03, 1.25552718e-04, 2.71537221e-04, 3.78257176e-04,\n",
       "        3.83471956e-05, 1.11677417e-03, 2.56276822e-04, 2.11716536e-04,\n",
       "        3.52672727e-04, 1.34383161e-03, 1.82300739e-04, 3.13137782e-04,\n",
       "        4.31612056e-04, 2.52350270e-04, 9.56002493e-05, 2.48424059e-04,\n",
       "        2.40881435e-04, 7.69080998e-04, 3.84955348e-04, 6.02178275e-04,\n",
       "        7.74666140e-04, 1.68110940e-04, 1.96629710e-04, 1.78092736e-04,\n",
       "        1.35212613e-03, 3.91336035e-04, 1.91572104e-04, 1.96617683e-04,\n",
       "        9.74390102e-05, 1.08147937e-03, 3.72302196e-04, 2.75101387e-04,\n",
       "        6.13163220e-04, 1.50660340e-03, 9.12163764e-04, 9.07963061e-05,\n",
       "        6.90178590e-05, 2.02308301e-04, 5.83394091e-04, 2.41232036e-04,\n",
       "        3.36367211e-04, 8.52248007e-04, 4.31629651e-04, 1.40951725e-04,\n",
       "        3.57503329e-04, 1.31604841e-03, 9.35487181e-05, 1.83094566e-04,\n",
       "        6.07074819e-04, 5.24479691e-05, 2.54389715e-04, 5.63442949e-04,\n",
       "        3.73766865e-04, 4.23235354e-05, 8.83684103e-04, 2.73223730e-03,\n",
       "        2.89499828e-04, 1.62926119e-03, 6.56375658e-05, 1.37454933e-03,\n",
       "        4.44569263e-04, 4.89054929e-04, 4.30777534e-04, 1.87690818e-03,\n",
       "        5.25899086e-04, 5.53192583e-03, 6.37694929e-03, 4.36278196e-04,\n",
       "        1.36665668e-03, 4.86491490e-03, 2.71924512e-03, 2.70232440e-04,\n",
       "        1.52549500e-03, 6.44843224e-04, 1.16899561e-03, 1.05820715e-03,\n",
       "        2.41089968e-03, 8.54030686e-04, 3.41788367e-04, 1.81893438e-03,\n",
       "        4.54609370e-04, 2.20077887e-04, 1.80278341e-03, 2.09263343e-04,\n",
       "        2.97078847e-03, 7.57815806e-04, 4.16289616e-04, 7.88365743e-04,\n",
       "        2.25494502e-04, 4.48761722e-04, 2.64210414e-04, 6.16669820e-04,\n",
       "        1.53360764e-04, 4.16560220e-04, 1.26046254e-04, 2.55821984e-04,\n",
       "        5.24272644e-03, 1.06498716e-04, 1.50708908e-04, 2.92434499e-04,\n",
       "        1.72085166e-03, 7.09614918e-03, 4.58408250e-04, 2.88039484e-04,\n",
       "        4.05797880e-04, 2.65938885e-04, 2.66187116e-04, 5.17743660e-04,\n",
       "        1.97413060e-04, 8.40135653e-05, 4.51583031e-04, 7.41606398e-04,\n",
       "        3.77641916e-04, 1.96384757e-04, 8.24616508e-04, 6.48727804e-04,\n",
       "        1.00826242e-03, 3.34684314e-03, 2.09541183e-04, 1.99947340e-03,\n",
       "        1.08484865e-04, 2.07448845e-03, 4.44809204e-03, 2.26185352e-03,\n",
       "        9.56256946e-05, 3.47651557e-04, 7.56054608e-04, 2.73129627e-04,\n",
       "        3.46633019e-04, 1.23466723e-04, 5.28598455e-05, 4.10282739e-04,\n",
       "        2.74842527e-04, 3.27972930e-04, 6.34908406e-04, 4.21061510e-04,\n",
       "        8.40848020e-04, 6.15915276e-04, 4.21135694e-04, 1.70687416e-04,\n",
       "        5.45757566e-03, 4.30074614e-04, 1.34010590e-03, 3.25413678e-03,\n",
       "        2.32766926e-03, 1.42195927e-04, 1.53628203e-03, 1.74647122e-03,\n",
       "        2.06164938e-03, 1.03688670e-03, 2.61964489e-03, 3.36051629e-04,\n",
       "        1.60065153e-03, 3.60530694e-04, 3.19811230e-03, 1.90339310e-04,\n",
       "        9.41303484e-04, 8.78583301e-05, 9.92440755e-05, 1.79073120e-04,\n",
       "        4.20264871e-04, 1.49355294e-04, 3.43546152e-04, 5.46856325e-04,\n",
       "        9.78377881e-04, 1.74903753e-04, 1.40571287e-04, 4.23914466e-04,\n",
       "        1.33059307e-03, 2.31180991e-04, 5.69898157e-04, 8.06446221e-03,\n",
       "        1.28137838e-03, 1.94363664e-03, 1.36223072e-03, 6.29590475e-05,\n",
       "        6.45159431e-04, 1.86150353e-03, 5.71983480e-03, 1.53606339e-04,\n",
       "        1.83563136e-03, 6.85821582e-04, 1.59578139e-03, 1.58597908e-03,\n",
       "        1.60777490e-03, 3.99615673e-04, 4.36915837e-04, 1.98138744e-04,\n",
       "        2.59499609e-04, 4.40996436e-04, 1.26597384e-03, 1.10154709e-04,\n",
       "        2.02472198e-03, 4.14039684e-04, 3.01220513e-04, 2.36337446e-03,\n",
       "        4.42166919e-04, 3.78378280e-04, 4.47690455e-04, 1.10571597e-04,\n",
       "        7.85440609e-04, 2.44713783e-03, 1.33411059e-03, 1.88913231e-03,\n",
       "        4.94256393e-03, 1.97114965e-04, 2.95304370e-03, 3.22919390e-04,\n",
       "        3.45830060e-04, 2.93167256e-03, 1.42498586e-04, 6.28590102e-04,\n",
       "        9.59918873e-04, 4.62809303e-04, 1.90298117e-04, 2.48873670e-04,\n",
       "        3.85803844e-04, 7.71114503e-04, 1.48687170e-04, 2.05929541e-03,\n",
       "        7.57927391e-04, 2.18902816e-04, 1.20722946e-04, 1.24231057e-03,\n",
       "        1.03186524e-03, 1.34162386e-04, 2.76128926e-04, 1.43275948e-03,\n",
       "        1.08166008e-04, 1.60658575e-04, 1.39572199e-04, 2.91888909e-04,\n",
       "        4.24409967e-04, 5.27033264e-04, 3.43368663e-03, 4.11912082e-03,\n",
       "        6.86484888e-05, 1.70809406e-03, 9.79994092e-04, 2.37810169e-04,\n",
       "        1.48630116e-03, 8.14114234e-05, 1.66986480e-04, 1.70840079e-03,\n",
       "        1.62415412e-04, 1.84406647e-04, 2.26037610e-03, 7.13375312e-04,\n",
       "        1.10229022e-03, 1.99741865e-03, 4.71569905e-04, 3.27797250e-04,\n",
       "        2.52152309e-03, 2.58328307e-04, 1.84594892e-04, 6.74978896e-04,\n",
       "        2.85357404e-04, 7.41222143e-04, 2.15461090e-03, 1.40265316e-03,\n",
       "        9.63944449e-05, 1.60932760e-04, 1.08544201e-03, 9.01781877e-04,\n",
       "        1.61526475e-03, 2.07861196e-03, 6.21241413e-05, 1.42150315e-04,\n",
       "        6.00971852e-05, 7.28235412e-04, 2.00752820e-03, 2.05638511e-04,\n",
       "        8.03277287e-04, 4.07263915e-03, 1.56716535e-04, 2.92333715e-04,\n",
       "        2.61548767e-04, 3.33326384e-04, 3.01233043e-04, 1.95574782e-04,\n",
       "        1.06785763e-04, 3.57422344e-04, 1.48673941e-04, 3.50102629e-04,\n",
       "        3.93009944e-04, 1.72629929e-03, 1.36997863e-04, 1.91859304e-04,\n",
       "        5.25722709e-04, 7.28631746e-04, 3.44487328e-04, 4.26173023e-04,\n",
       "        1.65151543e-04, 1.05142430e-03, 1.91842760e-04, 9.02984047e-05,\n",
       "        1.68226271e-04, 6.00017727e-04, 1.27589185e-04, 1.01577351e-04,\n",
       "        2.31239750e-04, 5.42626452e-04, 9.95833507e-05, 1.92379950e-04,\n",
       "        1.70009523e-03, 9.69874824e-05, 5.00940768e-04, 2.40258395e-04,\n",
       "        4.21305709e-04, 2.06071863e-04, 1.74021650e-03, 3.02982886e-04,\n",
       "        1.66123109e-03, 2.30668052e-04, 3.93980088e-04, 9.46584834e-04,\n",
       "        1.41912266e-04, 4.35315566e-03, 1.92324634e-03, 5.49874730e-04,\n",
       "        3.11726325e-04, 1.16469421e-03, 7.67763503e-05, 2.18758909e-04,\n",
       "        1.59636016e-03, 2.29001838e-04, 2.86008502e-05, 1.31737425e-03,\n",
       "        2.73398925e-04, 5.30944924e-04, 8.86608342e-04, 5.05133306e-04,\n",
       "        1.15844740e-03, 6.60185607e-04, 1.99514468e-04, 9.44493208e-05,\n",
       "        2.02580176e-03, 1.30824044e-04, 2.28510078e-04, 1.90446218e-04,\n",
       "        4.15699885e-04, 4.33452284e-04, 5.95246064e-04, 5.77701825e-04,\n",
       "        4.70169650e-04, 4.19147636e-04, 1.09029731e-04, 2.54519399e-05,\n",
       "        5.24390673e-04, 2.21090288e-04, 8.26870006e-05, 6.53629700e-04,\n",
       "        4.47553270e-05, 6.47325717e-05, 3.86559123e-04, 5.18885585e-04,\n",
       "        1.42891286e-03, 1.80653746e-04, 1.53523349e-04, 1.28982363e-03,\n",
       "        3.78379416e-04, 2.26046535e-04, 3.79770985e-04, 1.20753861e-03,\n",
       "        3.58734831e-03, 2.52635482e-04, 1.95095661e-04, 1.67505638e-04,\n",
       "        5.86175017e-04, 2.74067938e-04, 1.77027514e-04, 2.17114049e-04,\n",
       "        1.65810365e-03, 1.31624107e-04, 1.69649815e-04, 2.17579317e-04,\n",
       "        1.25617371e-04, 1.23699703e-03, 2.92202978e-04, 1.94711980e-04,\n",
       "        4.25457002e-04, 1.79925394e-03, 3.58112725e-04, 2.93491563e-04,\n",
       "        4.67778881e-04, 1.25716505e-04, 5.43720854e-04, 1.28750754e-03]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=[0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_child_weight': masked_array(data=[1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500}],\n",
       " 'split0_test_score': array([0.82828283, 0.85436893, 0.83809524, 0.84848485, 0.87378641,\n",
       "        0.86538462, 0.8627451 , 0.86538462, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84615385, 0.84313725, 0.8627451 , 0.83809524,\n",
       "        0.8627451 , 0.86538462, 0.85436893, 0.86      , 0.8627451 ,\n",
       "        0.86538462, 0.85148515, 0.84313725, 0.85436893, 0.8627451 ,\n",
       "        0.87378641, 0.87128713, 0.8627451 , 0.85148515, 0.83495146,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.8627451 , 0.85436893,\n",
       "        0.84313725, 0.8627451 , 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.85436893, 0.85436893, 0.85436893, 0.85148515,\n",
       "        0.82      , 0.85148515, 0.85436893, 0.83673469, 0.8627451 ,\n",
       "        0.86538462, 0.84848485, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.86538462, 0.86538462, 0.81188119, 0.85714286, 0.85714286,\n",
       "        0.81188119, 0.87378641, 0.85714286, 0.84848485, 0.8627451 ,\n",
       "        0.8627451 , 0.84      , 0.85436893, 0.81553398, 0.83495146,\n",
       "        0.86538462, 0.8490566 , 0.84      , 0.8627451 , 0.85148515,\n",
       "        0.84313725, 0.86538462, 0.85436893, 0.83495146, 0.85714286,\n",
       "        0.85714286, 0.85148515, 0.85148515, 0.84615385, 0.85436893,\n",
       "        0.84615385, 0.85436893, 0.83495146, 0.86792453, 0.85148515,\n",
       "        0.82828283, 0.84      , 0.83495146, 0.84848485, 0.85148515,\n",
       "        0.8627451 , 0.83673469, 0.85436893, 0.84615385, 0.82474227,\n",
       "        0.83168317, 0.8627451 , 0.82828283, 0.84313725, 0.8627451 ,\n",
       "        0.84      , 0.84313725, 0.87378641, 0.83673469, 0.86      ,\n",
       "        0.85436893, 0.83673469, 0.86      , 0.8627451 , 0.84      ,\n",
       "        0.8627451 , 0.86538462, 0.82828283, 0.8627451 , 0.8627451 ,\n",
       "        0.84      , 0.87378641, 0.87378641, 0.82828283, 0.85148515,\n",
       "        0.86538462, 0.83168317, 0.85148515, 0.85148515, 0.84      ,\n",
       "        0.87378641, 0.8627451 , 0.84      , 0.85436893, 0.85436893,\n",
       "        0.82105263, 0.83673469, 0.84848485, 0.82105263, 0.83673469,\n",
       "        0.84848485, 0.80851064, 0.83673469, 0.82828283, 0.8       ,\n",
       "        0.83673469, 0.84848485, 0.82105263, 0.83673469, 0.84848485,\n",
       "        0.8       , 0.83673469, 0.84      , 0.80851064, 0.83673469,\n",
       "        0.84848485, 0.82105263, 0.84848485, 0.84848485, 0.78723404,\n",
       "        0.84313725, 0.84      , 0.8       , 0.83673469, 0.83673469,\n",
       "        0.85416667, 0.82828283, 0.84      , 0.83673469, 0.84313725,\n",
       "        0.84313725, 0.83333333, 0.82828283, 0.82828283, 0.84536082,\n",
       "        0.82828283, 0.84      , 0.82828283, 0.84313725, 0.84313725,\n",
       "        0.82828283, 0.85436893, 0.83809524, 0.84848485, 0.87378641,\n",
       "        0.86538462, 0.8627451 , 0.86538462, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84615385, 0.84313725, 0.8627451 , 0.83809524,\n",
       "        0.8627451 , 0.86538462, 0.85436893, 0.86      , 0.8627451 ,\n",
       "        0.86538462, 0.85148515, 0.84313725, 0.85436893, 0.8627451 ,\n",
       "        0.87378641, 0.87128713, 0.8627451 , 0.85148515, 0.83495146,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.8627451 , 0.85436893,\n",
       "        0.84313725, 0.8627451 , 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.85436893, 0.85436893, 0.85436893, 0.85148515,\n",
       "        0.82      , 0.85148515, 0.85436893, 0.83673469, 0.8627451 ,\n",
       "        0.86538462, 0.84848485, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.86538462, 0.86538462, 0.81188119, 0.85714286, 0.85714286,\n",
       "        0.81188119, 0.87378641, 0.85714286, 0.84848485, 0.8627451 ,\n",
       "        0.8627451 , 0.84      , 0.85436893, 0.81553398, 0.83495146,\n",
       "        0.86538462, 0.8490566 , 0.84      , 0.8627451 , 0.85148515,\n",
       "        0.84313725, 0.86538462, 0.85436893, 0.83495146, 0.85714286,\n",
       "        0.85714286, 0.85148515, 0.85148515, 0.84615385, 0.85436893,\n",
       "        0.84615385, 0.85436893, 0.83495146, 0.86792453, 0.85148515,\n",
       "        0.82828283, 0.84      , 0.83495146, 0.84848485, 0.85148515,\n",
       "        0.8627451 , 0.83673469, 0.85436893, 0.84615385, 0.82474227,\n",
       "        0.83168317, 0.8627451 , 0.82828283, 0.84313725, 0.8627451 ,\n",
       "        0.84      , 0.84313725, 0.87378641, 0.83673469, 0.86      ,\n",
       "        0.85436893, 0.83673469, 0.86      , 0.8627451 , 0.84      ,\n",
       "        0.8627451 , 0.86538462, 0.82828283, 0.8627451 , 0.8627451 ,\n",
       "        0.84      , 0.87378641, 0.87378641, 0.82828283, 0.85148515,\n",
       "        0.86538462, 0.83168317, 0.85148515, 0.85148515, 0.84      ,\n",
       "        0.87378641, 0.8627451 , 0.84      , 0.85436893, 0.85436893,\n",
       "        0.82105263, 0.83673469, 0.84848485, 0.82105263, 0.83673469,\n",
       "        0.84848485, 0.80851064, 0.83673469, 0.82828283, 0.8       ,\n",
       "        0.83673469, 0.84848485, 0.82105263, 0.83673469, 0.84848485,\n",
       "        0.8       , 0.83673469, 0.84      , 0.80851064, 0.83673469,\n",
       "        0.84848485, 0.82105263, 0.84848485, 0.84848485, 0.78723404,\n",
       "        0.84313725, 0.84      , 0.8       , 0.83673469, 0.83673469,\n",
       "        0.85416667, 0.82828283, 0.84      , 0.83673469, 0.84313725,\n",
       "        0.84313725, 0.83333333, 0.82828283, 0.82828283, 0.84536082,\n",
       "        0.82828283, 0.84      , 0.82828283, 0.84313725, 0.84313725,\n",
       "        0.83168317, 0.83495146, 0.83809524, 0.84      , 0.8627451 ,\n",
       "        0.86538462, 0.84313725, 0.87378641, 0.86538462, 0.82352941,\n",
       "        0.85436893, 0.83809524, 0.83168317, 0.84615385, 0.85436893,\n",
       "        0.82352941, 0.85436893, 0.85436893, 0.82352941, 0.83495146,\n",
       "        0.83495146, 0.83168317, 0.84313725, 0.84313725, 0.82352941,\n",
       "        0.84615385, 0.86538462, 0.84615385, 0.83495146, 0.82692308,\n",
       "        0.85436893, 0.84313725, 0.84313725, 0.83495146, 0.85714286,\n",
       "        0.83809524, 0.84      , 0.84313725, 0.81553398, 0.8627451 ,\n",
       "        0.85148515, 0.8627451 , 0.85436893, 0.85714286, 0.85436893,\n",
       "        0.80392157, 0.82352941, 0.83809524, 0.85148515, 0.85148515,\n",
       "        0.8627451 , 0.84313725, 0.84313725, 0.86538462, 0.82352941,\n",
       "        0.83495146, 0.85436893, 0.84      , 0.85436893, 0.84615385,\n",
       "        0.82352941, 0.84313725, 0.84615385, 0.80769231, 0.84313725,\n",
       "        0.85148515, 0.82352941, 0.85436893, 0.84615385, 0.81553398,\n",
       "        0.83495146, 0.83809524, 0.80392157, 0.84313725, 0.84313725,\n",
       "        0.83168317, 0.84313725, 0.83495146, 0.81553398, 0.85436893,\n",
       "        0.85714286, 0.83168317, 0.84313725, 0.84313725, 0.8627451 ,\n",
       "        0.85148515, 0.84      , 0.82352941, 0.86538462, 0.84615385,\n",
       "        0.82      , 0.82352941, 0.83495146, 0.84536082, 0.84313725,\n",
       "        0.85148515, 0.81632653, 0.84615385, 0.85436893, 0.8       ,\n",
       "        0.83168317, 0.85436893, 0.80808081, 0.84313725, 0.85436893,\n",
       "        0.76767677, 0.85148515, 0.85148515, 0.8       , 0.83168317,\n",
       "        0.8627451 , 0.81632653, 0.84313725, 0.8627451 , 0.7755102 ,\n",
       "        0.82352941, 0.8627451 , 0.82828283, 0.83168317, 0.84313725,\n",
       "        0.82474227, 0.85148515, 0.85436893, 0.79591837, 0.84      ,\n",
       "        0.8627451 , 0.81632653, 0.85148515, 0.85148515, 0.82474227,\n",
       "        0.85148515, 0.8627451 , 0.79591837, 0.84313725, 0.86538462,\n",
       "        0.75555556, 0.8125    , 0.81188119, 0.76404494, 0.82105263,\n",
       "        0.84      , 0.74157303, 0.81632653, 0.84615385, 0.76595745,\n",
       "        0.79591837, 0.84      , 0.77777778, 0.80412371, 0.84848485,\n",
       "        0.71111111, 0.7628866 , 0.82352941, 0.76595745, 0.82      ,\n",
       "        0.83168317, 0.80434783, 0.81632653, 0.84      , 0.75268817,\n",
       "        0.79591837, 0.83168317, 0.77894737, 0.80808081, 0.82      ,\n",
       "        0.78723404, 0.81632653, 0.84      , 0.76086957, 0.78350515,\n",
       "        0.8       , 0.75      , 0.82828283, 0.82      , 0.7826087 ,\n",
       "        0.82474227, 0.82828283, 0.76923077, 0.79591837, 0.83168317]),\n",
       " 'split1_test_score': array([0.74509804, 0.72      , 0.7       , 0.74      , 0.74      ,\n",
       "        0.74509804, 0.74226804, 0.74      , 0.73267327, 0.70833333,\n",
       "        0.71428571, 0.73786408, 0.69387755, 0.7       , 0.7184466 ,\n",
       "        0.72916667, 0.72164948, 0.74226804, 0.68041237, 0.7       ,\n",
       "        0.7184466 , 0.66666667, 0.71428571, 0.75      , 0.72164948,\n",
       "        0.72727273, 0.74747475, 0.69387755, 0.7184466 , 0.71153846,\n",
       "        0.70103093, 0.70707071, 0.74285714, 0.72164948, 0.72      ,\n",
       "        0.76470588, 0.67368421, 0.7184466 , 0.71153846, 0.71287129,\n",
       "        0.72      , 0.74      , 0.70103093, 0.74747475, 0.76767677,\n",
       "        0.7       , 0.72      , 0.73267327, 0.71428571, 0.71428571,\n",
       "        0.74      , 0.72727273, 0.74      , 0.72727273, 0.70103093,\n",
       "        0.71428571, 0.72      , 0.71428571, 0.70103093, 0.71428571,\n",
       "        0.71428571, 0.71428571, 0.70707071, 0.68686869, 0.71428571,\n",
       "        0.71287129, 0.68041237, 0.70103093, 0.68686869, 0.71428571,\n",
       "        0.70707071, 0.71428571, 0.70103093, 0.72      , 0.7254902 ,\n",
       "        0.65979381, 0.71287129, 0.70588235, 0.70103093, 0.70707071,\n",
       "        0.74      , 0.6875    , 0.72727273, 0.7254902 , 0.68041237,\n",
       "        0.73267327, 0.7184466 , 0.70103093, 0.72      , 0.75247525,\n",
       "        0.71428571, 0.72727273, 0.74      , 0.67368421, 0.72      ,\n",
       "        0.71428571, 0.70833333, 0.72727273, 0.71428571, 0.6875    ,\n",
       "        0.70103093, 0.72727273, 0.6875    , 0.71428571, 0.72164948,\n",
       "        0.70833333, 0.71428571, 0.71428571, 0.70103093, 0.68041237,\n",
       "        0.70103093, 0.68686869, 0.70707071, 0.68686869, 0.70707071,\n",
       "        0.71428571, 0.69387755, 0.6875    , 0.69387755, 0.72      ,\n",
       "        0.65979381, 0.68041237, 0.70707071, 0.71428571, 0.70707071,\n",
       "        0.69387755, 0.70103093, 0.70103093, 0.74      , 0.67346939,\n",
       "        0.69387755, 0.71428571, 0.71428571, 0.70707071, 0.70103093,\n",
       "        0.68085106, 0.67346939, 0.7       , 0.66666667, 0.66666667,\n",
       "        0.70707071, 0.66666667, 0.68686869, 0.69387755, 0.66666667,\n",
       "        0.6875    , 0.68686869, 0.65979381, 0.66666667, 0.70707071,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.67346939, 0.68686869,\n",
       "        0.68686869, 0.67346939, 0.69387755, 0.70707071, 0.69387755,\n",
       "        0.68      , 0.70707071, 0.6875    , 0.7       , 0.7       ,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.69387755, 0.68686869,\n",
       "        0.69387755, 0.68041237, 0.68686869, 0.68686869, 0.66666667,\n",
       "        0.68      , 0.70707071, 0.70707071, 0.68      , 0.7       ,\n",
       "        0.74509804, 0.72      , 0.7       , 0.74      , 0.74      ,\n",
       "        0.74509804, 0.74226804, 0.74      , 0.73267327, 0.70833333,\n",
       "        0.71428571, 0.73786408, 0.69387755, 0.7       , 0.7184466 ,\n",
       "        0.72916667, 0.72164948, 0.74226804, 0.68041237, 0.7       ,\n",
       "        0.7184466 , 0.66666667, 0.71428571, 0.75      , 0.72164948,\n",
       "        0.72727273, 0.74747475, 0.69387755, 0.7184466 , 0.71153846,\n",
       "        0.70103093, 0.70707071, 0.74285714, 0.72164948, 0.72      ,\n",
       "        0.76470588, 0.67368421, 0.7184466 , 0.71153846, 0.71287129,\n",
       "        0.72      , 0.74      , 0.70103093, 0.74747475, 0.76767677,\n",
       "        0.7       , 0.72      , 0.73267327, 0.71428571, 0.71428571,\n",
       "        0.74      , 0.72727273, 0.74      , 0.72727273, 0.70103093,\n",
       "        0.71428571, 0.72      , 0.71428571, 0.70103093, 0.71428571,\n",
       "        0.71428571, 0.71428571, 0.70707071, 0.68686869, 0.71428571,\n",
       "        0.71287129, 0.68041237, 0.70103093, 0.68686869, 0.71428571,\n",
       "        0.70707071, 0.71428571, 0.70103093, 0.72      , 0.7254902 ,\n",
       "        0.65979381, 0.71287129, 0.70588235, 0.70103093, 0.70707071,\n",
       "        0.74      , 0.6875    , 0.72727273, 0.7254902 , 0.68041237,\n",
       "        0.73267327, 0.7184466 , 0.70103093, 0.72      , 0.75247525,\n",
       "        0.71428571, 0.72727273, 0.74      , 0.67368421, 0.72      ,\n",
       "        0.71428571, 0.70833333, 0.72727273, 0.71428571, 0.6875    ,\n",
       "        0.70103093, 0.72727273, 0.6875    , 0.71428571, 0.72164948,\n",
       "        0.70833333, 0.71428571, 0.71428571, 0.70103093, 0.68041237,\n",
       "        0.70103093, 0.68686869, 0.70707071, 0.68686869, 0.70707071,\n",
       "        0.71428571, 0.69387755, 0.6875    , 0.69387755, 0.72      ,\n",
       "        0.65979381, 0.68041237, 0.70707071, 0.71428571, 0.70707071,\n",
       "        0.69387755, 0.70103093, 0.70103093, 0.74      , 0.67346939,\n",
       "        0.69387755, 0.71428571, 0.71428571, 0.70707071, 0.70103093,\n",
       "        0.68085106, 0.67346939, 0.7       , 0.66666667, 0.66666667,\n",
       "        0.70707071, 0.66666667, 0.68686869, 0.69387755, 0.66666667,\n",
       "        0.6875    , 0.68686869, 0.65979381, 0.66666667, 0.70707071,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.67346939, 0.68686869,\n",
       "        0.68686869, 0.67346939, 0.69387755, 0.70707071, 0.69387755,\n",
       "        0.68      , 0.70707071, 0.6875    , 0.7       , 0.7       ,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.69387755, 0.68686869,\n",
       "        0.69387755, 0.68041237, 0.68686869, 0.68686869, 0.66666667,\n",
       "        0.68      , 0.70707071, 0.70707071, 0.68      , 0.7       ,\n",
       "        0.72      , 0.73267327, 0.74      , 0.7       , 0.72727273,\n",
       "        0.72      , 0.70103093, 0.70707071, 0.71428571, 0.69387755,\n",
       "        0.72727273, 0.69306931, 0.7       , 0.72164948, 0.71428571,\n",
       "        0.69387755, 0.68686869, 0.71428571, 0.67326733, 0.7       ,\n",
       "        0.72380952, 0.7       , 0.70833333, 0.70103093, 0.69387755,\n",
       "        0.7       , 0.70707071, 0.69306931, 0.72      , 0.70588235,\n",
       "        0.70707071, 0.72164948, 0.73267327, 0.69387755, 0.70707071,\n",
       "        0.74      , 0.67326733, 0.73786408, 0.71153846, 0.70707071,\n",
       "        0.70707071, 0.7254902 , 0.69387755, 0.69387755, 0.72      ,\n",
       "        0.66666667, 0.72      , 0.74509804, 0.68      , 0.72      ,\n",
       "        0.71428571, 0.69387755, 0.72      , 0.72727273, 0.67346939,\n",
       "        0.71428571, 0.72      , 0.67346939, 0.70707071, 0.71428571,\n",
       "        0.70707071, 0.68686869, 0.69387755, 0.67326733, 0.69387755,\n",
       "        0.68686869, 0.69306931, 0.72      , 0.71428571, 0.69387755,\n",
       "        0.69387755, 0.69387755, 0.67326733, 0.70707071, 0.72      ,\n",
       "        0.68686869, 0.71428571, 0.70707071, 0.69387755, 0.68686869,\n",
       "        0.70707071, 0.66      , 0.72      , 0.71153846, 0.7       ,\n",
       "        0.69387755, 0.72      , 0.69387755, 0.7       , 0.70707071,\n",
       "        0.66666667, 0.73267327, 0.72      , 0.65306122, 0.70707071,\n",
       "        0.70707071, 0.68686869, 0.72      , 0.72      , 0.65306122,\n",
       "        0.68686869, 0.71428571, 0.65306122, 0.70707071, 0.70707071,\n",
       "        0.66666667, 0.70707071, 0.70707071, 0.65306122, 0.67346939,\n",
       "        0.70707071, 0.65306122, 0.7       , 0.69387755, 0.67346939,\n",
       "        0.68686869, 0.68686869, 0.66666667, 0.69306931, 0.70707071,\n",
       "        0.66666667, 0.68686869, 0.70707071, 0.68686869, 0.68686869,\n",
       "        0.7       , 0.66666667, 0.7       , 0.71287129, 0.65306122,\n",
       "        0.68686869, 0.7       , 0.68686869, 0.69387755, 0.7       ,\n",
       "        0.60215054, 0.66666667, 0.66666667, 0.60215054, 0.66666667,\n",
       "        0.68686869, 0.60869565, 0.68      , 0.68686869, 0.63157895,\n",
       "        0.65306122, 0.66666667, 0.61052632, 0.65306122, 0.68      ,\n",
       "        0.63157895, 0.66666667, 0.70707071, 0.625     , 0.65306122,\n",
       "        0.68      , 0.63917526, 0.65306122, 0.69306931, 0.65979381,\n",
       "        0.68041237, 0.70707071, 0.63917526, 0.66      , 0.68627451,\n",
       "        0.63917526, 0.66666667, 0.68686869, 0.65979381, 0.69387755,\n",
       "        0.70707071, 0.63917526, 0.66      , 0.68627451, 0.63157895,\n",
       "        0.67346939, 0.68686869, 0.64583333, 0.69387755, 0.69387755]),\n",
       " 'split2_test_score': array([0.74509804, 0.74285714, 0.73584906, 0.75728155, 0.75      ,\n",
       "        0.75      , 0.75728155, 0.74285714, 0.75728155, 0.74285714,\n",
       "        0.74766355, 0.73584906, 0.75      , 0.74285714, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.75471698, 0.73076923, 0.72380952,\n",
       "        0.73584906, 0.74285714, 0.74285714, 0.75      , 0.73786408,\n",
       "        0.75728155, 0.74285714, 0.73076923, 0.73584906, 0.73584906,\n",
       "        0.75      , 0.74285714, 0.74285714, 0.75      , 0.74285714,\n",
       "        0.75471698, 0.73584906, 0.72222222, 0.72222222, 0.74285714,\n",
       "        0.74285714, 0.73584906, 0.73076923, 0.74285714, 0.73394495,\n",
       "        0.76470588, 0.72380952, 0.75471698, 0.76470588, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.73584906, 0.74285714, 0.73786408,\n",
       "        0.72380952, 0.72380952, 0.75728155, 0.75      , 0.74285714,\n",
       "        0.76190476, 0.75      , 0.75      , 0.71153846, 0.72897196,\n",
       "        0.73584906, 0.74285714, 0.76190476, 0.74285714, 0.74285714,\n",
       "        0.75      , 0.75      , 0.71698113, 0.72222222, 0.72897196,\n",
       "        0.73076923, 0.73584906, 0.74285714, 0.73076923, 0.74285714,\n",
       "        0.74285714, 0.71698113, 0.72222222, 0.72222222, 0.73076923,\n",
       "        0.73584906, 0.75      , 0.74285714, 0.73584906, 0.75471698,\n",
       "        0.75247525, 0.75728155, 0.75925926, 0.75247525, 0.75728155,\n",
       "        0.74285714, 0.74      , 0.75728155, 0.75      , 0.74509804,\n",
       "        0.72380952, 0.75471698, 0.74509804, 0.75      , 0.73584906,\n",
       "        0.73076923, 0.75      , 0.73786408, 0.73076923, 0.74766355,\n",
       "        0.72897196, 0.73076923, 0.74285714, 0.75      , 0.72380952,\n",
       "        0.75      , 0.73786408, 0.72380952, 0.71028037, 0.74074074,\n",
       "        0.73076923, 0.74285714, 0.74285714, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.71698113, 0.71028037, 0.72222222, 0.72380952,\n",
       "        0.73076923, 0.72897196, 0.72380952, 0.73786408, 0.73584906,\n",
       "        0.72727273, 0.74      , 0.75247525, 0.72727273, 0.74      ,\n",
       "        0.75247525, 0.72727273, 0.75247525, 0.75728155, 0.73267327,\n",
       "        0.74509804, 0.75247525, 0.74      , 0.75247525, 0.74509804,\n",
       "        0.74      , 0.75728155, 0.75      , 0.75728155, 0.73076923,\n",
       "        0.73076923, 0.75728155, 0.73786408, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.75      , 0.73267327, 0.73076923, 0.71698113,\n",
       "        0.75247525, 0.73076923, 0.73076923, 0.72380952, 0.74285714,\n",
       "        0.74285714, 0.73267327, 0.71698113, 0.71698113, 0.75247525,\n",
       "        0.72380952, 0.73076923, 0.72380952, 0.74285714, 0.74285714,\n",
       "        0.74509804, 0.74285714, 0.73584906, 0.75728155, 0.75      ,\n",
       "        0.75      , 0.75728155, 0.74285714, 0.75728155, 0.74285714,\n",
       "        0.74766355, 0.73584906, 0.75      , 0.74285714, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.75471698, 0.73076923, 0.72380952,\n",
       "        0.73584906, 0.74285714, 0.74285714, 0.75      , 0.73786408,\n",
       "        0.75728155, 0.74285714, 0.73076923, 0.73584906, 0.73584906,\n",
       "        0.75      , 0.74285714, 0.74285714, 0.75      , 0.74285714,\n",
       "        0.75471698, 0.73584906, 0.72222222, 0.72222222, 0.74285714,\n",
       "        0.74285714, 0.73584906, 0.73076923, 0.74285714, 0.73394495,\n",
       "        0.76470588, 0.72380952, 0.75471698, 0.76470588, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.73584906, 0.74285714, 0.73786408,\n",
       "        0.72380952, 0.72380952, 0.75728155, 0.75      , 0.74285714,\n",
       "        0.76190476, 0.75      , 0.75      , 0.71153846, 0.72897196,\n",
       "        0.73584906, 0.74285714, 0.76190476, 0.74285714, 0.74285714,\n",
       "        0.75      , 0.75      , 0.71698113, 0.72222222, 0.72897196,\n",
       "        0.73076923, 0.73584906, 0.74285714, 0.73076923, 0.74285714,\n",
       "        0.74285714, 0.71698113, 0.72222222, 0.72222222, 0.73076923,\n",
       "        0.73584906, 0.75      , 0.74285714, 0.73584906, 0.75471698,\n",
       "        0.75247525, 0.75728155, 0.75925926, 0.75247525, 0.75728155,\n",
       "        0.74285714, 0.74      , 0.75728155, 0.75      , 0.74509804,\n",
       "        0.72380952, 0.75471698, 0.74509804, 0.75      , 0.73584906,\n",
       "        0.73076923, 0.75      , 0.73786408, 0.73076923, 0.74766355,\n",
       "        0.72897196, 0.73076923, 0.74285714, 0.75      , 0.72380952,\n",
       "        0.75      , 0.73786408, 0.72380952, 0.71028037, 0.74074074,\n",
       "        0.73076923, 0.74285714, 0.74285714, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.71698113, 0.71028037, 0.72222222, 0.72380952,\n",
       "        0.73076923, 0.72897196, 0.72380952, 0.73786408, 0.73584906,\n",
       "        0.72727273, 0.74      , 0.75247525, 0.72727273, 0.74      ,\n",
       "        0.75247525, 0.72727273, 0.75247525, 0.75728155, 0.73267327,\n",
       "        0.74509804, 0.75247525, 0.74      , 0.75247525, 0.74509804,\n",
       "        0.74      , 0.75728155, 0.75      , 0.75728155, 0.73076923,\n",
       "        0.73076923, 0.75728155, 0.73786408, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.75      , 0.73267327, 0.73076923, 0.71698113,\n",
       "        0.75247525, 0.73076923, 0.73076923, 0.72380952, 0.74285714,\n",
       "        0.74285714, 0.73267327, 0.71698113, 0.71698113, 0.75247525,\n",
       "        0.72380952, 0.73076923, 0.72380952, 0.74285714, 0.74285714,\n",
       "        0.75247525, 0.75471698, 0.72897196, 0.73076923, 0.74074074,\n",
       "        0.75      , 0.75      , 0.73584906, 0.75      , 0.72380952,\n",
       "        0.73786408, 0.74074074, 0.73584906, 0.75471698, 0.74766355,\n",
       "        0.75      , 0.75      , 0.75471698, 0.73786408, 0.73394495,\n",
       "        0.73394495, 0.73584906, 0.76190476, 0.73584906, 0.73786408,\n",
       "        0.74285714, 0.74285714, 0.72380952, 0.73394495, 0.72222222,\n",
       "        0.73076923, 0.72897196, 0.74285714, 0.73076923, 0.74285714,\n",
       "        0.75471698, 0.71153846, 0.72897196, 0.72222222, 0.74285714,\n",
       "        0.74074074, 0.73584906, 0.72380952, 0.73584906, 0.75471698,\n",
       "        0.75247525, 0.75925926, 0.75229358, 0.73267327, 0.74766355,\n",
       "        0.73584906, 0.74509804, 0.75      , 0.74285714, 0.75247525,\n",
       "        0.7254902 , 0.73584906, 0.73267327, 0.74285714, 0.75471698,\n",
       "        0.74509804, 0.75      , 0.76190476, 0.73267327, 0.74285714,\n",
       "        0.72897196, 0.7254902 , 0.75471698, 0.74766355, 0.73786408,\n",
       "        0.74285714, 0.74285714, 0.7184466 , 0.74545455, 0.73394495,\n",
       "        0.71698113, 0.75471698, 0.74285714, 0.73786408, 0.73584906,\n",
       "        0.74285714, 0.7047619 , 0.73873874, 0.73394495, 0.73076923,\n",
       "        0.74766355, 0.73584906, 0.73076923, 0.73584906, 0.74285714,\n",
       "        0.74      , 0.76923077, 0.74766355, 0.74      , 0.75471698,\n",
       "        0.75471698, 0.74      , 0.76190476, 0.75471698, 0.74509804,\n",
       "        0.75      , 0.76635514, 0.74509804, 0.75      , 0.74285714,\n",
       "        0.73267327, 0.75      , 0.76190476, 0.74509804, 0.73584906,\n",
       "        0.75925926, 0.74509804, 0.74285714, 0.75471698, 0.73786408,\n",
       "        0.74285714, 0.76190476, 0.73267327, 0.74285714, 0.74545455,\n",
       "        0.74509804, 0.73786408, 0.75471698, 0.73076923, 0.74285714,\n",
       "        0.74285714, 0.74509804, 0.75471698, 0.74545455, 0.73786408,\n",
       "        0.74285714, 0.74766355, 0.73076923, 0.74285714, 0.74285714,\n",
       "        0.70833333, 0.74      , 0.75247525, 0.70833333, 0.73267327,\n",
       "        0.74509804, 0.75510204, 0.74      , 0.76470588, 0.74      ,\n",
       "        0.75728155, 0.75728155, 0.74747475, 0.74509804, 0.74509804,\n",
       "        0.74747475, 0.73786408, 0.75      , 0.76      , 0.75      ,\n",
       "        0.75      , 0.75247525, 0.74509804, 0.73076923, 0.74      ,\n",
       "        0.73786408, 0.74285714, 0.76      , 0.75      , 0.72222222,\n",
       "        0.74      , 0.73786408, 0.73076923, 0.74509804, 0.73786408,\n",
       "        0.75      , 0.74      , 0.74285714, 0.72897196, 0.74      ,\n",
       "        0.73786408, 0.72380952, 0.74509804, 0.73076923, 0.75      ]),\n",
       " 'split3_test_score': array([0.76470588, 0.78095238, 0.78846154, 0.78846154, 0.78846154,\n",
       "        0.78846154, 0.78846154, 0.80769231, 0.80769231, 0.8       ,\n",
       "        0.78095238, 0.78095238, 0.8       , 0.78846154, 0.79245283,\n",
       "        0.77669903, 0.80769231, 0.81904762, 0.80769231, 0.78095238,\n",
       "        0.77358491, 0.81553398, 0.78846154, 0.77669903, 0.7961165 ,\n",
       "        0.81553398, 0.80392157, 0.80769231, 0.78846154, 0.78846154,\n",
       "        0.80769231, 0.78846154, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.81553398, 0.78846154, 0.78095238, 0.77669903, 0.8       ,\n",
       "        0.76923077, 0.77358491, 0.80769231, 0.7961165 , 0.78846154,\n",
       "        0.76470588, 0.78846154, 0.78095238, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.77227723, 0.79245283, 0.8       , 0.78846154,\n",
       "        0.78846154, 0.77358491, 0.77669903, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.80769231, 0.80769231, 0.78095238, 0.78095238,\n",
       "        0.77777778, 0.7961165 , 0.78846154, 0.78095238, 0.7961165 ,\n",
       "        0.80769231, 0.7961165 , 0.81553398, 0.78095238, 0.76923077,\n",
       "        0.80769231, 0.8       , 0.78095238, 0.80769231, 0.80769231,\n",
       "        0.7961165 , 0.80769231, 0.78095238, 0.76923077, 0.80769231,\n",
       "        0.79245283, 0.8       , 0.80769231, 0.80769231, 0.7961165 ,\n",
       "        0.77227723, 0.78846154, 0.78095238, 0.76470588, 0.78846154,\n",
       "        0.8       , 0.77227723, 0.78846154, 0.80769231, 0.78431373,\n",
       "        0.8       , 0.78095238, 0.77669903, 0.79245283, 0.8       ,\n",
       "        0.77669903, 0.8       , 0.80769231, 0.80392157, 0.7961165 ,\n",
       "        0.7961165 , 0.78095238, 0.8       , 0.79245283, 0.80392157,\n",
       "        0.80769231, 0.80769231, 0.81553398, 0.80392157, 0.78095238,\n",
       "        0.80769231, 0.8       , 0.79245283, 0.80392157, 0.80769231,\n",
       "        0.80769231, 0.80769231, 0.78846154, 0.78095238, 0.80769231,\n",
       "        0.79245283, 0.78504673, 0.80392157, 0.80769231, 0.80769231,\n",
       "        0.75510204, 0.76470588, 0.77669903, 0.75510204, 0.76470588,\n",
       "        0.76923077, 0.74      , 0.76470588, 0.76923077, 0.77227723,\n",
       "        0.76470588, 0.78846154, 0.76470588, 0.77669903, 0.76923077,\n",
       "        0.75728155, 0.77669903, 0.76923077, 0.80392157, 0.7961165 ,\n",
       "        0.80392157, 0.80392157, 0.7961165 , 0.7961165 , 0.77669903,\n",
       "        0.78431373, 0.7961165 , 0.80392157, 0.78431373, 0.81553398,\n",
       "        0.79207921, 0.7961165 , 0.80769231, 0.77669903, 0.7961165 ,\n",
       "        0.80769231, 0.80392157, 0.80392157, 0.81553398, 0.79207921,\n",
       "        0.80769231, 0.8       , 0.77669903, 0.7961165 , 0.81553398,\n",
       "        0.76470588, 0.78095238, 0.78846154, 0.78846154, 0.78846154,\n",
       "        0.78846154, 0.78846154, 0.80769231, 0.80769231, 0.8       ,\n",
       "        0.78095238, 0.78095238, 0.8       , 0.78846154, 0.79245283,\n",
       "        0.77669903, 0.80769231, 0.81904762, 0.80769231, 0.78095238,\n",
       "        0.77358491, 0.81553398, 0.78846154, 0.77669903, 0.7961165 ,\n",
       "        0.81553398, 0.80392157, 0.80769231, 0.78846154, 0.78846154,\n",
       "        0.80769231, 0.78846154, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.81553398, 0.78846154, 0.78095238, 0.77669903, 0.8       ,\n",
       "        0.76923077, 0.77358491, 0.80769231, 0.7961165 , 0.78846154,\n",
       "        0.76470588, 0.78846154, 0.78095238, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.77227723, 0.79245283, 0.8       , 0.78846154,\n",
       "        0.78846154, 0.77358491, 0.77669903, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.80769231, 0.80769231, 0.78095238, 0.78095238,\n",
       "        0.77777778, 0.7961165 , 0.78846154, 0.78095238, 0.7961165 ,\n",
       "        0.80769231, 0.7961165 , 0.81553398, 0.78095238, 0.76923077,\n",
       "        0.80769231, 0.8       , 0.78095238, 0.80769231, 0.80769231,\n",
       "        0.7961165 , 0.80769231, 0.78095238, 0.76923077, 0.80769231,\n",
       "        0.79245283, 0.8       , 0.80769231, 0.80769231, 0.7961165 ,\n",
       "        0.77227723, 0.78846154, 0.78095238, 0.76470588, 0.78846154,\n",
       "        0.8       , 0.77227723, 0.78846154, 0.80769231, 0.78431373,\n",
       "        0.8       , 0.78095238, 0.77669903, 0.79245283, 0.8       ,\n",
       "        0.77669903, 0.8       , 0.80769231, 0.80392157, 0.7961165 ,\n",
       "        0.7961165 , 0.78095238, 0.8       , 0.79245283, 0.80392157,\n",
       "        0.80769231, 0.80769231, 0.81553398, 0.80392157, 0.78095238,\n",
       "        0.80769231, 0.8       , 0.79245283, 0.80392157, 0.80769231,\n",
       "        0.80769231, 0.80769231, 0.78846154, 0.78095238, 0.80769231,\n",
       "        0.79245283, 0.78504673, 0.80392157, 0.80769231, 0.80769231,\n",
       "        0.75510204, 0.76470588, 0.77669903, 0.75510204, 0.76470588,\n",
       "        0.76923077, 0.74      , 0.76470588, 0.76923077, 0.77227723,\n",
       "        0.76470588, 0.78846154, 0.76470588, 0.77669903, 0.76923077,\n",
       "        0.75728155, 0.77669903, 0.76923077, 0.80392157, 0.7961165 ,\n",
       "        0.80392157, 0.80392157, 0.7961165 , 0.7961165 , 0.77669903,\n",
       "        0.78431373, 0.7961165 , 0.80392157, 0.78431373, 0.81553398,\n",
       "        0.79207921, 0.7961165 , 0.80769231, 0.77669903, 0.7961165 ,\n",
       "        0.80769231, 0.80392157, 0.80392157, 0.81553398, 0.79207921,\n",
       "        0.80769231, 0.8       , 0.77669903, 0.7961165 , 0.81553398,\n",
       "        0.77669903, 0.79245283, 0.79245283, 0.77669903, 0.78095238,\n",
       "        0.78846154, 0.77669903, 0.81132075, 0.7961165 , 0.80769231,\n",
       "        0.8       , 0.78095238, 0.8       , 0.78846154, 0.79245283,\n",
       "        0.7961165 , 0.81904762, 0.82692308, 0.82692308, 0.79245283,\n",
       "        0.79245283, 0.82692308, 0.79245283, 0.78504673, 0.80769231,\n",
       "        0.81904762, 0.81904762, 0.81553398, 0.78846154, 0.77358491,\n",
       "        0.82692308, 0.77358491, 0.78504673, 0.81553398, 0.80769231,\n",
       "        0.81132075, 0.80769231, 0.79245283, 0.77777778, 0.80769231,\n",
       "        0.78504673, 0.78504673, 0.82692308, 0.81132075, 0.8       ,\n",
       "        0.77669903, 0.81132075, 0.78095238, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.78431373, 0.8       , 0.7961165 , 0.7961165 ,\n",
       "        0.8       , 0.78095238, 0.78431373, 0.8       , 0.78846154,\n",
       "        0.78431373, 0.80769231, 0.80769231, 0.80392157, 0.78095238,\n",
       "        0.78504673, 0.7961165 , 0.79245283, 0.78504673, 0.78846154,\n",
       "        0.8       , 0.81904762, 0.82692308, 0.78846154, 0.77358491,\n",
       "        0.80769231, 0.78846154, 0.78504673, 0.80769231, 0.80769231,\n",
       "        0.8       , 0.80769231, 0.78095238, 0.78095238, 0.80769231,\n",
       "        0.78504673, 0.79245283, 0.81553398, 0.81904762, 0.81132075,\n",
       "        0.78      , 0.76923077, 0.78095238, 0.76470588, 0.78095238,\n",
       "        0.79245283, 0.76470588, 0.78095238, 0.78095238, 0.77669903,\n",
       "        0.80373832, 0.78095238, 0.77669903, 0.79245283, 0.79245283,\n",
       "        0.77669903, 0.79245283, 0.8       , 0.7961165 , 0.79245283,\n",
       "        0.79245283, 0.7961165 , 0.79245283, 0.79245283, 0.77669903,\n",
       "        0.79245283, 0.8       , 0.7961165 , 0.78846154, 0.78095238,\n",
       "        0.77669903, 0.8       , 0.79245283, 0.7961165 , 0.8       ,\n",
       "        0.81132075, 0.78846154, 0.78095238, 0.79245283, 0.78846154,\n",
       "        0.79245283, 0.7962963 , 0.7961165 , 0.81132075, 0.8       ,\n",
       "        0.77419355, 0.78787879, 0.76470588, 0.77419355, 0.78787879,\n",
       "        0.76470588, 0.75789474, 0.78787879, 0.76470588, 0.77083333,\n",
       "        0.78      , 0.77669903, 0.7628866 , 0.78      , 0.77669903,\n",
       "        0.7755102 , 0.78      , 0.77669903, 0.82828283, 0.76470588,\n",
       "        0.78846154, 0.78787879, 0.76470588, 0.78846154, 0.7755102 ,\n",
       "        0.76      , 0.78846154, 0.79166667, 0.77227723, 0.8       ,\n",
       "        0.7628866 , 0.76470588, 0.79245283, 0.7628866 , 0.76      ,\n",
       "        0.8       , 0.8125    , 0.77669903, 0.78846154, 0.7628866 ,\n",
       "        0.76923077, 0.8       , 0.7628866 , 0.78      , 0.80769231]),\n",
       " 'split4_test_score': array([0.73684211, 0.8       , 0.81188119, 0.74747475, 0.76      ,\n",
       "        0.75      , 0.72164948, 0.73469388, 0.75510204, 0.75510204,\n",
       "        0.8       , 0.77083333, 0.74      , 0.7628866 , 0.7628866 ,\n",
       "        0.71428571, 0.74468085, 0.75      , 0.81632653, 0.83673469,\n",
       "        0.82828283, 0.79591837, 0.77083333, 0.78350515, 0.74226804,\n",
       "        0.76595745, 0.75      , 0.79591837, 0.81632653, 0.81632653,\n",
       "        0.78787879, 0.81632653, 0.78787879, 0.76      , 0.75789474,\n",
       "        0.7755102 , 0.79591837, 0.82828283, 0.79591837, 0.78      ,\n",
       "        0.78787879, 0.7755102 , 0.78      , 0.75789474, 0.7628866 ,\n",
       "        0.73684211, 0.8       , 0.79591837, 0.72916667, 0.76767677,\n",
       "        0.77083333, 0.72916667, 0.70833333, 0.73469388, 0.75510204,\n",
       "        0.78350515, 0.8       , 0.72727273, 0.7628866 , 0.75      ,\n",
       "        0.74      , 0.72164948, 0.76595745, 0.7755102 , 0.80412371,\n",
       "        0.80412371, 0.74226804, 0.7628866 , 0.75      , 0.75510204,\n",
       "        0.74226804, 0.75789474, 0.79591837, 0.82      , 0.81632653,\n",
       "        0.78350515, 0.7628866 , 0.78350515, 0.7755102 , 0.75789474,\n",
       "        0.75789474, 0.79207921, 0.82      , 0.82828283, 0.79591837,\n",
       "        0.78      , 0.78787879, 0.76767677, 0.75789474, 0.75789474,\n",
       "        0.73684211, 0.75510204, 0.8       , 0.72340426, 0.73267327,\n",
       "        0.75510204, 0.71578947, 0.72727273, 0.70833333, 0.74226804,\n",
       "        0.78787879, 0.8       , 0.73469388, 0.77227723, 0.78      ,\n",
       "        0.71428571, 0.70707071, 0.71428571, 0.77083333, 0.78787879,\n",
       "        0.8125    , 0.75      , 0.76470588, 0.7755102 , 0.75      ,\n",
       "        0.72164948, 0.75789474, 0.80412371, 0.80808081, 0.80808081,\n",
       "        0.77083333, 0.81188119, 0.8       , 0.75      , 0.7628866 ,\n",
       "        0.74468085, 0.80808081, 0.80392157, 0.82828283, 0.78350515,\n",
       "        0.81188119, 0.8       , 0.75789474, 0.7628866 , 0.74468085,\n",
       "        0.73333333, 0.73684211, 0.73684211, 0.73333333, 0.73684211,\n",
       "        0.73684211, 0.73333333, 0.72340426, 0.72916667, 0.74725275,\n",
       "        0.77083333, 0.75789474, 0.73333333, 0.73684211, 0.75      ,\n",
       "        0.73333333, 0.7628866 , 0.75510204, 0.76595745, 0.77083333,\n",
       "        0.77083333, 0.7311828 , 0.7628866 , 0.7628866 , 0.7173913 ,\n",
       "        0.75      , 0.76767677, 0.77419355, 0.78350515, 0.8       ,\n",
       "        0.76595745, 0.75789474, 0.76767677, 0.70967742, 0.74226804,\n",
       "        0.7628866 , 0.79569892, 0.76767677, 0.79207921, 0.76595745,\n",
       "        0.7755102 , 0.8       , 0.7173913 , 0.7628866 , 0.7755102 ,\n",
       "        0.73684211, 0.8       , 0.81188119, 0.74747475, 0.76      ,\n",
       "        0.75      , 0.72164948, 0.73469388, 0.75510204, 0.75510204,\n",
       "        0.8       , 0.77083333, 0.74      , 0.7628866 , 0.7628866 ,\n",
       "        0.71428571, 0.74468085, 0.75      , 0.81632653, 0.83673469,\n",
       "        0.82828283, 0.79591837, 0.77083333, 0.78350515, 0.74226804,\n",
       "        0.76595745, 0.75      , 0.79591837, 0.81632653, 0.81632653,\n",
       "        0.78787879, 0.81632653, 0.78787879, 0.76      , 0.75789474,\n",
       "        0.7755102 , 0.79591837, 0.82828283, 0.79591837, 0.78      ,\n",
       "        0.78787879, 0.7755102 , 0.78      , 0.75789474, 0.7628866 ,\n",
       "        0.73684211, 0.8       , 0.79591837, 0.72916667, 0.76767677,\n",
       "        0.77083333, 0.72916667, 0.70833333, 0.73469388, 0.75510204,\n",
       "        0.78350515, 0.8       , 0.72727273, 0.7628866 , 0.75      ,\n",
       "        0.74      , 0.72164948, 0.76595745, 0.7755102 , 0.80412371,\n",
       "        0.80412371, 0.74226804, 0.7628866 , 0.75      , 0.75510204,\n",
       "        0.74226804, 0.75789474, 0.79591837, 0.82      , 0.81632653,\n",
       "        0.78350515, 0.7628866 , 0.78350515, 0.7755102 , 0.75789474,\n",
       "        0.75789474, 0.79207921, 0.82      , 0.82828283, 0.79591837,\n",
       "        0.78      , 0.78787879, 0.76767677, 0.75789474, 0.75789474,\n",
       "        0.73684211, 0.75510204, 0.8       , 0.72340426, 0.73267327,\n",
       "        0.75510204, 0.71578947, 0.72727273, 0.70833333, 0.74226804,\n",
       "        0.78787879, 0.8       , 0.73469388, 0.77227723, 0.78      ,\n",
       "        0.71428571, 0.70707071, 0.71428571, 0.77083333, 0.78787879,\n",
       "        0.8125    , 0.75      , 0.76470588, 0.7755102 , 0.75      ,\n",
       "        0.72164948, 0.75789474, 0.80412371, 0.80808081, 0.80808081,\n",
       "        0.77083333, 0.81188119, 0.8       , 0.75      , 0.7628866 ,\n",
       "        0.74468085, 0.80808081, 0.80392157, 0.82828283, 0.78350515,\n",
       "        0.81188119, 0.8       , 0.75789474, 0.7628866 , 0.74468085,\n",
       "        0.73333333, 0.73684211, 0.73684211, 0.73333333, 0.73684211,\n",
       "        0.73684211, 0.73333333, 0.72340426, 0.72916667, 0.74725275,\n",
       "        0.77083333, 0.75789474, 0.73333333, 0.73684211, 0.75      ,\n",
       "        0.73333333, 0.7628866 , 0.75510204, 0.76595745, 0.77083333,\n",
       "        0.77083333, 0.7311828 , 0.7628866 , 0.7628866 , 0.7173913 ,\n",
       "        0.75      , 0.76767677, 0.77419355, 0.78350515, 0.8       ,\n",
       "        0.76595745, 0.75789474, 0.76767677, 0.70967742, 0.74226804,\n",
       "        0.7628866 , 0.79569892, 0.76767677, 0.79207921, 0.76595745,\n",
       "        0.7755102 , 0.8       , 0.7173913 , 0.7628866 , 0.7755102 ,\n",
       "        0.74226804, 0.79207921, 0.80392157, 0.72727273, 0.76      ,\n",
       "        0.76      , 0.73469388, 0.72      , 0.72727273, 0.74226804,\n",
       "        0.79207921, 0.79166667, 0.72164948, 0.76      , 0.75510204,\n",
       "        0.74747475, 0.73267327, 0.74226804, 0.76470588, 0.79591837,\n",
       "        0.80412371, 0.72727273, 0.75247525, 0.76767677, 0.73469388,\n",
       "        0.75510204, 0.7755102 , 0.78431373, 0.78787879, 0.78350515,\n",
       "        0.79207921, 0.76767677, 0.76767677, 0.76767677, 0.76      ,\n",
       "        0.7755102 , 0.79207921, 0.79591837, 0.78350515, 0.79207921,\n",
       "        0.76767677, 0.77227723, 0.7755102 , 0.76767677, 0.7755102 ,\n",
       "        0.73684211, 0.78431373, 0.8       , 0.74226804, 0.77669903,\n",
       "        0.76470588, 0.73684211, 0.72      , 0.72727273, 0.72164948,\n",
       "        0.78      , 0.8125    , 0.72727273, 0.76470588, 0.74747475,\n",
       "        0.73469388, 0.7254902 , 0.72727273, 0.73469388, 0.78431373,\n",
       "        0.82474227, 0.72727273, 0.74509804, 0.75247525, 0.74226804,\n",
       "        0.74509804, 0.73469388, 0.76767677, 0.8       , 0.79591837,\n",
       "        0.74747475, 0.75247525, 0.7755102 , 0.7628866 , 0.74      ,\n",
       "        0.73469388, 0.76      , 0.78      , 0.80808081, 0.78      ,\n",
       "        0.75247525, 0.76767677, 0.74226804, 0.74      , 0.73469388,\n",
       "        0.74157303, 0.75510204, 0.77227723, 0.72527473, 0.74      ,\n",
       "        0.7961165 , 0.72340426, 0.73469388, 0.73267327, 0.73913043,\n",
       "        0.75247525, 0.77227723, 0.72340426, 0.73267327, 0.76470588,\n",
       "        0.71578947, 0.71287129, 0.7254902 , 0.73684211, 0.77669903,\n",
       "        0.78787879, 0.72916667, 0.76923077, 0.76470588, 0.72340426,\n",
       "        0.73267327, 0.75247525, 0.72916667, 0.76470588, 0.80808081,\n",
       "        0.73684211, 0.75      , 0.76470588, 0.71578947, 0.73267327,\n",
       "        0.74      , 0.75      , 0.76470588, 0.81188119, 0.75      ,\n",
       "        0.77669903, 0.78      , 0.73684211, 0.73267327, 0.74747475,\n",
       "        0.71264368, 0.73333333, 0.72916667, 0.68235294, 0.73333333,\n",
       "        0.72164948, 0.6744186 , 0.72340426, 0.72164948, 0.69565217,\n",
       "        0.71578947, 0.73469388, 0.70967742, 0.72916667, 0.73469388,\n",
       "        0.65934066, 0.73469388, 0.72727273, 0.7032967 , 0.75      ,\n",
       "        0.73469388, 0.70967742, 0.74226804, 0.72727273, 0.68131868,\n",
       "        0.73469388, 0.74747475, 0.7173913 , 0.74226804, 0.74747475,\n",
       "        0.72527473, 0.73684211, 0.75247525, 0.68131868, 0.73469388,\n",
       "        0.75510204, 0.7173913 , 0.74226804, 0.78431373, 0.7173913 ,\n",
       "        0.73684211, 0.76470588, 0.68888889, 0.70833333, 0.76      ]),\n",
       " 'mean_test_score': array([0.76400538, 0.77963569, 0.7748574 , 0.77634054, 0.78244959,\n",
       "        0.77978884, 0.77448114, 0.77812559, 0.78362676, 0.76759514,\n",
       "        0.77945412, 0.77433054, 0.76540296, 0.77139008, 0.76954606,\n",
       "        0.7665793 , 0.78082263, 0.78408031, 0.77904009, 0.78084834,\n",
       "        0.7843096 , 0.77449226, 0.771915  , 0.78291462, 0.77212864,\n",
       "        0.78796642, 0.78310812, 0.77820051, 0.78211378, 0.77742541,\n",
       "        0.78186942, 0.78181697, 0.78178288, 0.78041738, 0.77424746,\n",
       "        0.79072086, 0.77133165, 0.77860826, 0.76508514, 0.77801947,\n",
       "        0.77486713, 0.77586262, 0.77477228, 0.77974241, 0.780891  ,\n",
       "        0.75725077, 0.77675124, 0.78372599, 0.7643184 , 0.77460189,\n",
       "        0.7829359 , 0.76838147, 0.76620083, 0.77404167, 0.75886795,\n",
       "        0.77508931, 0.77655581, 0.75748404, 0.76955188, 0.77134771,\n",
       "        0.76330664, 0.77348278, 0.77757266, 0.76067092, 0.77821577,\n",
       "        0.77867339, 0.76033081, 0.77373055, 0.75524244, 0.76866257,\n",
       "        0.77448313, 0.77347071, 0.77389288, 0.78118394, 0.77830092,\n",
       "        0.76497955, 0.77539831, 0.77351319, 0.76999083, 0.77453155,\n",
       "        0.77880225, 0.77114756, 0.7803865 , 0.77827597, 0.77383224,\n",
       "        0.7774258 , 0.78213886, 0.77084172, 0.77787213, 0.78253772,\n",
       "        0.76083262, 0.77362357, 0.78303262, 0.75255089, 0.7699803 ,\n",
       "        0.774998  , 0.75462695, 0.7709315 , 0.76529304, 0.75678441,\n",
       "        0.76888048, 0.78513744, 0.75445475, 0.77443061, 0.78004873,\n",
       "        0.75401746, 0.76289874, 0.76958284, 0.76865795, 0.77441424,\n",
       "        0.77859767, 0.757065  , 0.77492675, 0.77351536, 0.76496036,\n",
       "        0.77127452, 0.77254266, 0.77185001, 0.77578108, 0.78250381,\n",
       "        0.76181774, 0.78178742, 0.78323342, 0.76545187, 0.77339977,\n",
       "        0.77089849, 0.77309367, 0.77103591, 0.78458852, 0.76569527,\n",
       "        0.78055344, 0.7782099 , 0.76798231, 0.77397652, 0.76872442,\n",
       "        0.74352236, 0.75035041, 0.76290025, 0.74068548, 0.74898987,\n",
       "        0.76282074, 0.73515667, 0.75283775, 0.75556787, 0.74377398,\n",
       "        0.76097439, 0.76683701, 0.74377713, 0.75388355, 0.76397687,\n",
       "        0.74081685, 0.76005371, 0.7642807 , 0.76182812, 0.76426449,\n",
       "        0.76817553, 0.75738159, 0.76784592, 0.76906558, 0.7426132 ,\n",
       "        0.76006162, 0.7721728 , 0.75965768, 0.76706456, 0.77384996,\n",
       "        0.76762959, 0.75594599, 0.7706418 , 0.74815964, 0.76224953,\n",
       "        0.77009017, 0.76920789, 0.7607462 , 0.76794917, 0.76450788,\n",
       "        0.76305897, 0.77556799, 0.75065068, 0.7649995 , 0.77540772,\n",
       "        0.76400538, 0.77963569, 0.7748574 , 0.77634054, 0.78244959,\n",
       "        0.77978884, 0.77448114, 0.77812559, 0.78362676, 0.76759514,\n",
       "        0.77945412, 0.77433054, 0.76540296, 0.77139008, 0.76954606,\n",
       "        0.7665793 , 0.78082263, 0.78408031, 0.77904009, 0.78084834,\n",
       "        0.7843096 , 0.77449226, 0.771915  , 0.78291462, 0.77212864,\n",
       "        0.78796642, 0.78310812, 0.77820051, 0.78211378, 0.77742541,\n",
       "        0.78186942, 0.78181697, 0.78178288, 0.78041738, 0.77424746,\n",
       "        0.79072086, 0.77133165, 0.77860826, 0.76508514, 0.77801947,\n",
       "        0.77486713, 0.77586262, 0.77477228, 0.77974241, 0.780891  ,\n",
       "        0.75725077, 0.77675124, 0.78372599, 0.7643184 , 0.77460189,\n",
       "        0.7829359 , 0.76838147, 0.76620083, 0.77404167, 0.75886795,\n",
       "        0.77508931, 0.77655581, 0.75748404, 0.76955188, 0.77134771,\n",
       "        0.76330664, 0.77348278, 0.77757266, 0.76067092, 0.77821577,\n",
       "        0.77867339, 0.76033081, 0.77373055, 0.75524244, 0.76866257,\n",
       "        0.77448313, 0.77347071, 0.77389288, 0.78118394, 0.77830092,\n",
       "        0.76497955, 0.77539831, 0.77351319, 0.76999083, 0.77453155,\n",
       "        0.77880225, 0.77114756, 0.7803865 , 0.77827597, 0.77383224,\n",
       "        0.7774258 , 0.78213886, 0.77084172, 0.77787213, 0.78253772,\n",
       "        0.76083262, 0.77362357, 0.78303262, 0.75255089, 0.7699803 ,\n",
       "        0.774998  , 0.75462695, 0.7709315 , 0.76529304, 0.75678441,\n",
       "        0.76888048, 0.78513744, 0.75445475, 0.77443061, 0.78004873,\n",
       "        0.75401746, 0.76289874, 0.76958284, 0.76865795, 0.77441424,\n",
       "        0.77859767, 0.757065  , 0.77492675, 0.77351536, 0.76496036,\n",
       "        0.77127452, 0.77254266, 0.77185001, 0.77578108, 0.78250381,\n",
       "        0.76181774, 0.78178742, 0.78323342, 0.76545187, 0.77339977,\n",
       "        0.77089849, 0.77309367, 0.77103591, 0.78458852, 0.76569527,\n",
       "        0.78055344, 0.7782099 , 0.76798231, 0.77397652, 0.76872442,\n",
       "        0.74352236, 0.75035041, 0.76290025, 0.74068548, 0.74898987,\n",
       "        0.76282074, 0.73515667, 0.75283775, 0.75556787, 0.74377398,\n",
       "        0.76097439, 0.76683701, 0.74377713, 0.75388355, 0.76397687,\n",
       "        0.74081685, 0.76005371, 0.7642807 , 0.76182812, 0.76426449,\n",
       "        0.76817553, 0.75738159, 0.76784592, 0.76906558, 0.7426132 ,\n",
       "        0.76006162, 0.7721728 , 0.75965768, 0.76706456, 0.77384996,\n",
       "        0.76762959, 0.75594599, 0.7706418 , 0.74815964, 0.76224953,\n",
       "        0.77009017, 0.76920789, 0.7607462 , 0.76794917, 0.76450788,\n",
       "        0.76305897, 0.77556799, 0.75065068, 0.7649995 , 0.77540772,\n",
       "        0.7646251 , 0.78137475, 0.78068832, 0.7549482 , 0.77434219,\n",
       "        0.77676923, 0.76111222, 0.76960539, 0.77061191, 0.75823537,\n",
       "        0.78231699, 0.76890487, 0.75783634, 0.77419637, 0.77277461,\n",
       "        0.76219964, 0.7685917 , 0.77851255, 0.76525796, 0.77145352,\n",
       "        0.7778565 , 0.76434561, 0.77166069, 0.76654815, 0.75953145,\n",
       "        0.77263213, 0.78197406, 0.77257608, 0.77304735, 0.76242354,\n",
       "        0.78224223, 0.76700408, 0.77427823, 0.7685618 , 0.7749526 ,\n",
       "        0.78392864, 0.76491546, 0.7796689 , 0.76211552, 0.78248889,\n",
       "        0.77040402, 0.77628166, 0.77489786, 0.7731734 , 0.78091922,\n",
       "        0.74732092, 0.77968463, 0.78328785, 0.7566251 , 0.77766011,\n",
       "        0.77320946, 0.76065374, 0.76662745, 0.77178074, 0.75344801,\n",
       "        0.77094547, 0.78073407, 0.75154582, 0.77380053, 0.77021857,\n",
       "        0.75894115, 0.76263769, 0.76738024, 0.75044967, 0.76902761,\n",
       "        0.77542296, 0.75309563, 0.77332736, 0.76912502, 0.75560104,\n",
       "        0.76335684, 0.76571429, 0.75804707, 0.77682481, 0.7733171 ,\n",
       "        0.75814001, 0.77061535, 0.76908725, 0.7635709 , 0.7649558 ,\n",
       "        0.76835292, 0.75282748, 0.77256567, 0.77553077, 0.77624133,\n",
       "        0.76610965, 0.77119573, 0.76119564, 0.77205626, 0.76841927,\n",
       "        0.74964794, 0.76995325, 0.77116892, 0.74568053, 0.76517546,\n",
       "        0.78036843, 0.74626107, 0.76874097, 0.76854231, 0.74279775,\n",
       "        0.76495308, 0.77764788, 0.74126867, 0.76506681, 0.7722911 ,\n",
       "        0.73190104, 0.76277599, 0.76919016, 0.74622357, 0.76203069,\n",
       "        0.78188134, 0.74795379, 0.7695356 , 0.77369967, 0.73738939,\n",
       "        0.75567627, 0.77279876, 0.75058119, 0.76415541, 0.77693914,\n",
       "        0.75000962, 0.76524358, 0.77466307, 0.74509245, 0.76047982,\n",
       "        0.7713846 , 0.75331055, 0.77037208, 0.782829  , 0.75082582,\n",
       "        0.77007257, 0.77734099, 0.74930298, 0.76477319, 0.7711433 ,\n",
       "        0.71057533, 0.74807576, 0.74497913, 0.70621506, 0.74832094,\n",
       "        0.75166442, 0.70753681, 0.74952191, 0.75681676, 0.72080438,\n",
       "        0.74041012, 0.75506823, 0.72166857, 0.74228993, 0.75699516,\n",
       "        0.70500313, 0.73642224, 0.75691438, 0.7365074 , 0.74755342,\n",
       "        0.75696772, 0.73871091, 0.74429194, 0.75591456, 0.72186217,\n",
       "        0.74177774, 0.76350946, 0.73743612, 0.74652522, 0.7551943 ,\n",
       "        0.73091412, 0.74448105, 0.7605132 , 0.72199334, 0.74198813,\n",
       "        0.76243455, 0.73181331, 0.75002141, 0.76160435, 0.72689311,\n",
       "        0.74842972, 0.76073338, 0.72238753, 0.7417797 , 0.76865061]),\n",
       " 'std_test_score': array([0.03342125, 0.04671311, 0.05035899, 0.03967073, 0.04845098,\n",
       "        0.04556326, 0.04921518, 0.05113133, 0.0476812 , 0.04344504,\n",
       "        0.04752719, 0.04006319, 0.05145488, 0.05407431, 0.04246104,\n",
       "        0.05245849, 0.05087268, 0.0445761 , 0.06453408, 0.06264225,\n",
       "        0.05533205, 0.06434057, 0.04361894, 0.03823989, 0.05176992,\n",
       "        0.05144846, 0.04938575, 0.05948372, 0.04943102, 0.04693404,\n",
       "        0.05439546, 0.05216075, 0.04083384, 0.04963371, 0.04710256,\n",
       "        0.03336942, 0.06332203, 0.05183794, 0.04171809, 0.04857362,\n",
       "        0.04594921, 0.04255765, 0.05444399, 0.0417523 , 0.03935366,\n",
       "        0.03935172, 0.0495708 , 0.04146004, 0.04274684, 0.05152798,\n",
       "        0.04450407, 0.04398401, 0.05180568, 0.05239356, 0.03869535,\n",
       "        0.05427629, 0.05372103, 0.03494889, 0.05068547, 0.04965832,\n",
       "        0.03446874, 0.05995587, 0.05119433, 0.05692732, 0.0535461 ,\n",
       "        0.05268854, 0.05412272, 0.04952217, 0.04278299, 0.04231923,\n",
       "        0.05576307, 0.04587463, 0.05502193, 0.05545975, 0.04920663,\n",
       "        0.06404778, 0.05354081, 0.04940963, 0.04891942, 0.05245051,\n",
       "        0.04398141, 0.06026391, 0.05065318, 0.05122161, 0.06115906,\n",
       "        0.04168308, 0.0461709 , 0.04719052, 0.05387559, 0.0380083 ,\n",
       "        0.0387105 , 0.03843047, 0.03287785, 0.05730716, 0.04698021,\n",
       "        0.051845  , 0.04673031, 0.04747127, 0.05369581, 0.04587757,\n",
       "        0.04880264, 0.04590226, 0.04670411, 0.04304487, 0.05020125,\n",
       "        0.0492211 , 0.0518866 , 0.06234154, 0.04876816, 0.05921006,\n",
       "        0.05599441, 0.05017324, 0.05216032, 0.05724539, 0.0498344 ,\n",
       "        0.05631358, 0.05909398, 0.05578056, 0.06386929, 0.05048987,\n",
       "        0.06272338, 0.06556465, 0.05656317, 0.04355977, 0.05108064,\n",
       "        0.05947658, 0.0532833 , 0.05735701, 0.04954849, 0.05976983,\n",
       "        0.062939  , 0.05325115, 0.0477797 , 0.05196764, 0.05494501,\n",
       "        0.04569593, 0.05266616, 0.04951622, 0.04975833, 0.0546839 ,\n",
       "        0.04747507, 0.0450791 , 0.04979544, 0.04467616, 0.04484343,\n",
       "        0.04796101, 0.05256453, 0.05212687, 0.0552884 , 0.04681725,\n",
       "        0.04090497, 0.05457818, 0.04317195, 0.04857181, 0.05181057,\n",
       "        0.056115  , 0.05281594, 0.05233868, 0.04977414, 0.03517592,\n",
       "        0.05347302, 0.044551  , 0.04412023, 0.04740625, 0.05488853,\n",
       "        0.05862601, 0.05559632, 0.04862756, 0.05230754, 0.05319284,\n",
       "        0.05167531, 0.05520563, 0.05262435, 0.05596161, 0.05833895,\n",
       "        0.05448051, 0.04907981, 0.04567519, 0.05439382, 0.05087884,\n",
       "        0.03342125, 0.04671311, 0.05035899, 0.03967073, 0.04845098,\n",
       "        0.04556326, 0.04921518, 0.05113133, 0.0476812 , 0.04344504,\n",
       "        0.04752719, 0.04006319, 0.05145488, 0.05407431, 0.04246104,\n",
       "        0.05245849, 0.05087268, 0.0445761 , 0.06453408, 0.06264225,\n",
       "        0.05533205, 0.06434057, 0.04361894, 0.03823989, 0.05176992,\n",
       "        0.05144846, 0.04938575, 0.05948372, 0.04943102, 0.04693404,\n",
       "        0.05439546, 0.05216075, 0.04083384, 0.04963371, 0.04710256,\n",
       "        0.03336942, 0.06332203, 0.05183794, 0.04171809, 0.04857362,\n",
       "        0.04594921, 0.04255765, 0.05444399, 0.0417523 , 0.03935366,\n",
       "        0.03935172, 0.0495708 , 0.04146004, 0.04274684, 0.05152798,\n",
       "        0.04450407, 0.04398401, 0.05180568, 0.05239356, 0.03869535,\n",
       "        0.05427629, 0.05372103, 0.03494889, 0.05068547, 0.04965832,\n",
       "        0.03446874, 0.05995587, 0.05119433, 0.05692732, 0.0535461 ,\n",
       "        0.05268854, 0.05412272, 0.04952217, 0.04278299, 0.04231923,\n",
       "        0.05576307, 0.04587463, 0.05502193, 0.05545975, 0.04920663,\n",
       "        0.06404778, 0.05354081, 0.04940963, 0.04891942, 0.05245051,\n",
       "        0.04398141, 0.06026391, 0.05065318, 0.05122161, 0.06115906,\n",
       "        0.04168308, 0.0461709 , 0.04719052, 0.05387559, 0.0380083 ,\n",
       "        0.0387105 , 0.03843047, 0.03287785, 0.05730716, 0.04698021,\n",
       "        0.051845  , 0.04673031, 0.04747127, 0.05369581, 0.04587757,\n",
       "        0.04880264, 0.04590226, 0.04670411, 0.04304487, 0.05020125,\n",
       "        0.0492211 , 0.0518866 , 0.06234154, 0.04876816, 0.05921006,\n",
       "        0.05599441, 0.05017324, 0.05216032, 0.05724539, 0.0498344 ,\n",
       "        0.05631358, 0.05909398, 0.05578056, 0.06386929, 0.05048987,\n",
       "        0.06272338, 0.06556465, 0.05656317, 0.04355977, 0.05108064,\n",
       "        0.05947658, 0.0532833 , 0.05735701, 0.04954849, 0.05976983,\n",
       "        0.062939  , 0.05325115, 0.0477797 , 0.05196764, 0.05494501,\n",
       "        0.04569593, 0.05266616, 0.04951622, 0.04975833, 0.0546839 ,\n",
       "        0.04747507, 0.0450791 , 0.04979544, 0.04467616, 0.04484343,\n",
       "        0.04796101, 0.05256453, 0.05212687, 0.0552884 , 0.04681725,\n",
       "        0.04090497, 0.05457818, 0.04317195, 0.04857181, 0.05181057,\n",
       "        0.056115  , 0.05281594, 0.05233868, 0.04977414, 0.03517592,\n",
       "        0.05347302, 0.044551  , 0.04412023, 0.04740625, 0.05488853,\n",
       "        0.05862601, 0.05559632, 0.04862756, 0.05230754, 0.05319284,\n",
       "        0.05167531, 0.05520563, 0.05262435, 0.05596161, 0.05833895,\n",
       "        0.05448051, 0.04907981, 0.04567519, 0.05439382, 0.05087884,\n",
       "        0.03816129, 0.03518592, 0.04075258, 0.04914624, 0.04776831,\n",
       "        0.04941648, 0.04775401, 0.06341077, 0.05496931, 0.04958089,\n",
       "        0.04605519, 0.04897918, 0.04973751, 0.04176968, 0.04776757,\n",
       "        0.04459878, 0.06036219, 0.05310885, 0.05729076, 0.04813312,\n",
       "        0.04245504, 0.05436377, 0.0447545 , 0.04784204, 0.04860457,\n",
       "        0.05297003, 0.05570314, 0.0567386 , 0.04155167, 0.04368582,\n",
       "        0.05584914, 0.04322847, 0.03903681, 0.05219343, 0.05232356,\n",
       "        0.03616148, 0.06237612, 0.04188921, 0.03925779, 0.05371559,\n",
       "        0.0483467 , 0.04854556, 0.06032737, 0.05695128, 0.04513514,\n",
       "        0.04627783, 0.03721314, 0.0337867 , 0.05666147, 0.04448453,\n",
       "        0.05136025, 0.05026203, 0.04815016, 0.05320229, 0.05318021,\n",
       "        0.04538596, 0.04926359, 0.05646089, 0.05034035, 0.04468789,\n",
       "        0.04070753, 0.05619731, 0.05453729, 0.05031366, 0.04939743,\n",
       "        0.0605145 , 0.04866982, 0.0467345 , 0.04456927, 0.04236491,\n",
       "        0.04909074, 0.05427463, 0.05603018, 0.04673237, 0.04422357,\n",
       "        0.05431653, 0.04320362, 0.04282084, 0.04508954, 0.05897238,\n",
       "        0.05370971, 0.06357209, 0.0424579 , 0.0479248 , 0.05721302,\n",
       "        0.05175175, 0.04258794, 0.05030686, 0.06074379, 0.05184144,\n",
       "        0.05082291, 0.02993929, 0.03830383, 0.06216933, 0.04570842,\n",
       "        0.04792113, 0.04321683, 0.04409874, 0.04765355, 0.04998556,\n",
       "        0.04986715, 0.04487354, 0.05259948, 0.04792679, 0.04964042,\n",
       "        0.03950308, 0.0538643 , 0.05204479, 0.0531972 , 0.0538901 ,\n",
       "        0.05058668, 0.05720173, 0.04793628, 0.05494283, 0.03815762,\n",
       "        0.04771691, 0.05787219, 0.05644809, 0.04616912, 0.04741514,\n",
       "        0.05184756, 0.05616092, 0.0484463 , 0.04391026, 0.05364264,\n",
       "        0.05802794, 0.05058097, 0.04882764, 0.04886972, 0.05763106,\n",
       "        0.05447267, 0.05389363, 0.04186004, 0.05448764, 0.05680812,\n",
       "        0.0597256 , 0.05028869, 0.04754808, 0.06223935, 0.05289919,\n",
       "        0.05121671, 0.05800556, 0.04804207, 0.0533574 , 0.05196918,\n",
       "        0.0513209 , 0.05642011, 0.06001586, 0.05174424, 0.05536379,\n",
       "        0.05345758, 0.03865305, 0.04059513, 0.06837951, 0.05382093,\n",
       "        0.05107438, 0.05943977, 0.0527892 , 0.05202275, 0.04393994,\n",
       "        0.0376682 , 0.0427533 , 0.05519104, 0.04892661, 0.04920166,\n",
       "        0.0504471 , 0.04843146, 0.05236624, 0.0429899 , 0.0297756 ,\n",
       "        0.03491666, 0.056032  , 0.05486682, 0.04772114, 0.052434  ,\n",
       "        0.04923666, 0.05087227, 0.04760869, 0.0398153 , 0.04798134]),\n",
       " 'rank_test_score': array([366,  80, 154, 128,  35,  74, 167, 105,  16, 315,  82, 174, 335,\n",
       "        233, 276, 324,  61,  11,  84,  59,   9, 163, 226,  27, 223,   3,\n",
       "         21, 103,  41, 118,  45,  47,  51,  67, 177,   1, 238,  90, 342,\n",
       "        107, 152, 132, 156,  76,  57, 431, 124,  14, 359, 159,  25, 302,\n",
       "        327, 180, 421, 144, 126, 427, 274, 236, 373, 200, 114, 406,  99,\n",
       "         88, 411, 191, 449, 293, 165, 202, 184,  54,  95, 347, 142, 198,\n",
       "        266, 161,  86, 244,  69,  97, 188, 116,  39, 254, 109,  30, 401,\n",
       "        194,  23, 468, 268, 146, 454, 250, 337, 439, 288,   5, 456, 169,\n",
       "         72, 458, 379, 272, 295, 171,  92, 433, 149, 196, 349, 240, 218,\n",
       "        228, 134,  32, 394,  49,  19, 333, 204, 252, 210, 247,   7, 331,\n",
       "         65, 101, 307, 182, 291, 506, 477, 377, 518, 484, 381, 526, 465,\n",
       "        447, 504, 399, 321, 502, 460, 368, 516, 415, 361, 392, 363, 305,\n",
       "        429, 311, 284, 509, 413, 221, 417, 318, 186, 313, 442, 256, 488,\n",
       "        387, 263, 279, 403, 309, 356, 375, 136, 473, 345, 140, 366,  80,\n",
       "        154, 128,  35,  74, 167, 105,  16, 315,  82, 174, 335, 233, 276,\n",
       "        324,  61,  11,  84,  59,   9, 163, 226,  27, 223,   3,  21, 103,\n",
       "         41, 118,  45,  47,  51,  67, 177,   1, 238,  90, 342, 107, 152,\n",
       "        132, 156,  76,  57, 431, 124,  14, 359, 159,  25, 302, 327, 180,\n",
       "        421, 144, 126, 427, 274, 236, 373, 200, 114, 406,  99,  88, 411,\n",
       "        191, 449, 293, 165, 202, 184,  54,  95, 347, 142, 198, 266, 161,\n",
       "         86, 244,  69,  97, 188, 116,  39, 254, 109,  30, 401, 194,  23,\n",
       "        468, 268, 146, 454, 250, 337, 439, 288,   5, 456, 169,  72, 458,\n",
       "        379, 272, 295, 171,  92, 433, 149, 196, 349, 240, 218, 228, 134,\n",
       "         32, 394,  49,  19, 333, 204, 252, 210, 247,   7, 331,  65, 101,\n",
       "        307, 182, 291, 506, 477, 377, 518, 484, 381, 526, 465, 447, 504,\n",
       "        399, 321, 502, 460, 368, 516, 415, 361, 392, 363, 305, 429, 311,\n",
       "        284, 509, 413, 221, 417, 318, 186, 313, 442, 256, 488, 387, 263,\n",
       "        279, 403, 309, 356, 375, 136, 473, 345, 140, 355,  53,  64, 453,\n",
       "        173, 123, 398, 271, 259, 423,  37, 287, 426, 179, 214, 389, 298,\n",
       "         94, 339, 232, 111, 358, 231, 326, 419, 215,  43, 216, 212, 386,\n",
       "         38, 320, 176, 299, 148,  13, 353,  79, 390,  34, 260, 130, 151,\n",
       "        209,  56, 493,  78,  18, 441, 112, 208, 408, 323, 230, 462, 249,\n",
       "         63, 471, 190, 262, 420, 384, 317, 476, 286, 139, 464, 206, 282,\n",
       "        446, 372, 330, 425, 122, 207, 424, 258, 283, 370, 351, 304, 467,\n",
       "        217, 138, 131, 329, 242, 397, 225, 301, 481, 270, 243, 497, 341,\n",
       "         71, 495, 290, 300, 508, 352, 113, 515, 344, 220, 528, 383, 281,\n",
       "        496, 391,  44, 491, 278, 193, 523, 445, 213, 475, 365, 121, 480,\n",
       "        340, 158, 498, 410, 235, 463, 261,  29, 472, 265, 120, 483, 354,\n",
       "        246, 537, 490, 499, 539, 487, 470, 538, 482, 438, 536, 520, 452,\n",
       "        535, 511, 435, 540, 525, 437, 524, 492, 436, 521, 501, 444, 534,\n",
       "        514, 371, 522, 494, 451, 530, 500, 409, 533, 512, 385, 529, 479,\n",
       "        396, 531, 486, 405, 532, 513, 297], dtype=int32)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5,\n",
       " 'learning_rate': 0.07,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 3,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7907208606102268"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.775785\n",
      "F1: 0.652778\n"
     ]
    }
   ],
   "source": [
    "preds = gsearch1.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1bf90b50>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot feature importance\n",
    "plot_importance(alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fare': 231,\n",
       " 'Parch': 91,\n",
       " 'male': 42,\n",
       " 'Pclass': 81,\n",
       " 'SibSp': 86,\n",
       " 'S': 43,\n",
       " 'Q': 37,\n",
       " 'youngin': 27,\n",
       " 'Age': 150}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle list object\n",
    " \n",
    "model_pickle_path = 'xg_boost_model.pkl'\n",
    "\n",
    "# Create an variable to pickle and open it in write mode\n",
    "model_pickle = open(model_pickle_path, 'wb')\n",
    "pickle.dump(gsearch1.best_estimator_, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the saved XGboost model pickle\n",
    "xgboost_model_pkl = open(model_pickle_path, 'rb')\n",
    "xgboost_model = pickle.load(xgboost_model_pkl)\n",
    "print(\"Loaded XGboost model :: \", xgboost_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "214.1875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
